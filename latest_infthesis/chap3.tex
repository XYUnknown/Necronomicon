% begin patch To deal with latex problem typesetting ! in listing environment
\newcommand*\bang{!}
% end patch

\Chapter{Oxidising Remote Procedure Calls}{A Universal Method Invocation Library for Rust}
\label{chap3}

\begin{tikzpicture}[color=black,
                   transform shape,
                   every node/.style={inner sep=0pt}]
% \node[minimum size=\framesize,fill=white](vecbox){};
% \node[text width=\framesize,align=center](Text){%
%     `Then you should encode what you mean', the March Hare went on.
% \\
% `I do', Programmer hastily replied; `at least --- at least I mean what I encode --- that's the same thing, you know.'
% \\
% `Not the same thing a bit', said the Hatter.};
\node[minimum width=\framesize, minimum height=0.60 *\framesize, fill=white](vecbox){};
\node[anchor=north west] at (vecbox.north west){% 
\pgfornament[width=0.1*\framesize]{61}};
\node[anchor=north east] at (vecbox.north east){% 
\pgfornament[width=0.1*\framesize,symmetry=v]{61}};
\node[anchor=south west] at (vecbox.south west){% 
\pgfornament[width=0.1*\framesize,symmetry=h]{61}};
\node[anchor=south east] at (vecbox.south east){% 
\pgfornament[width=0.1*\framesize,symmetry=c]{61}};
\node[anchor=north] at (vecbox.north){% 
\pgfornament[width=0.25*\framesize]{88}
% \pgfornament[width=0.1*\framesize]{15}
% \pgfornament[width=0.1*\framesize]{16}
\pgfornament[width=0.25*\framesize]{88}
\pgfornament[width=0.25*\framesize]{88}
};

\node[anchor=south] at (vecbox.south){% 
% \pgfornament[width=0.25*\framesize]{88}
% \pgfornament[width=0.1*\framesize,symmetry=h]{15}
\pgfornament[width=0.25*\framesize]{88}
\pgfornament[width=0.25*\framesize]{88}
\pgfornament[width=0.25*\framesize]{88}
% \pgfornament[width=0.1*\framesize,symmetry=h]{16}
% \pgfornament[width=0.25*\framesize]{88}
};
% \node[anchor=south] at (vecbox.south){% 
% \pgfornament[width=0.6*\framesize]{46}};
% \node[anchor=north,rotate=90] at (vecbox.west){% 
% \pgfornament[width=0.6*\framesize,symmetry=h]{46}};
% \node[anchor=north,rotate=-90] at (vecbox.east){% 
% \pgfornament[width=0.6*\framesize,symmetry=h]{46}};
\node[text width=0.85\framesize,align=justify] at (vecbox.center){%
The building is circular. The apartments of the prisoners occupy the circumference. You may call them, if you please, the cells. The apartment of the inspector occupies the centre, you may call it if you please the inspector’s lodge. To cut off from each prisoner the view of every other, the partitions are carried on a few feet beyond the grating into the intermediate area, such projecting parts I call the protracted partitions. These windows of the inspector’s lodge open into the intermediate area, in the form of doors, in as many places as shall be deemed necessary to admit of his communicating readily with any of the cells. \\
\rightline{--- Jeremy Bentham ``Panopticon or the Inspection-House"}};
\end{tikzpicture}
\noindent
\begin{center}
\vspace{0.3em}
\begin{tikzpicture}[color=black,
                   transform shape,
                   every node/.style={inner sep=0pt}]
\node[minimum width=0.35\framesize, minimum height=0.05*\framesize, fill=white](vecbox){};
\node[anchor=west] at (vecbox.west){\pgfornament[width=0.08*\framesize]{16}};
\node[anchor=east] at (vecbox.east){\pgfornament[width=0.08*\framesize]{15}};
\node[inner sep=6pt] (text) at (vecbox.center){\textbf{\Large Prologue}};
\end{tikzpicture}
\vspace{-0.7em}
\end{center}
% \section{Prologue}
% \label{chap3:prologue}
\lettrine{R}{ecall} that in chapter~\ref{chap1}, we have briefly discussed the conceptual question that we would like to ask: How to intuitively understand \emph{distributed programs} using the same conceptual model as \emph{monolithic programs}? It would be beneficial to view a monolithic programs as an abstraction of distributed program, specifying the intended behaviours of the invocations and resource usage in the distributed program while abstracting away message-passing details, since it would make the process of migrating monolithic programs into a distributed setting straightforward and simplify the process of implementing a distributed system.

In this chapter, we discuss in detail our design, implementation and formalisation of a \emph{universal method invocation} (UMI) library in Rust, which supports location transparency, allowing a monolithic program to be migrated into a distributed design with minimal syntactic modification and preserves the semantics of the original program.

As a metaphorical illustration, the core idea of our attempt for conceptually modelling and understanding distributed programs as monolithic programs resembles a \emph{panopticon}, where the distributed resource management is governed by a monolithic design of Rust's ownership and borrow checking system.

\section{Introduction}
\label{chap3:introduction}
Distributed computing has extensive application in areas such as cloud computing, big data processing, web services, and blockchain systems, driving the development of modern, large-scale, and resilient software systems. Distributed systems offer many significant advantages such as scalability, fault tolerance, resource sharing, and geographical distribution. 

However, comparing to monolithic systems, distributed systems are more complex and challenging to design, implement, test and debug due to the necessity for coordination, synchronisation, and communication among distributed components. Therefore, it is a common practice to implement a system, a monolithic design is initially adopted as the implementation and deployment of a monolithic system is straightforward. Such a monolithic system needs to later be re-structured and migrated into a distributed design when it needs to be expanded into a larger scale. However, it still requires non-trivial effort to be put into the migration of a monolithic system into a distributed design.

To address these issues in the design and implementation of a distributed system as well as migrating a monolithic systems into a distributed setting, we propose our design of a UMI library in Rust. The design of the UMI library share the same underlying idea of the \emph{remote procedural call} (RPC)~\citep{10.5555/910306}, where a method invocation on an object can be executed on a different node within the same network, abstracting over the underlying message-passing details. Such a design allows programmers to model a distributed system focusing on \emph{what} functional features are required instead of \emph{how} these functional features are achieved via complicated network communications. Moreover, it allows applications to be migrated from a monolithic design to a distributed architecture without massive changes to source code or the needs of high-level expertise in microservices. In addition, by choosing Rust as the language for implementing the UMI framework, we are able to avoid distributed memory management hassles like distributed garbage collection while extending Rust's memory safety and data-racing free guarantees into the distributed setting.

In summary, we make following contributions:
\begin{itemize}
    \item We provide a usable \emph{Rust implementation of the UMI framework} (section~\ref{chap3:implementation}).
    \item We formalise the \emph{structural operational semantics} for a core calculus of monolithic and distributed Rust programs (section~\ref{chap3:semantics}).
    \item We prove a \emph{location transparency theorem}: With the UMI framework, when a monolithic program is deployed to multiple nodes, its semantics is preserved (section~\ref{chap3:semantics:loc-transp}).
\end{itemize}

Before stepping into the detailed discussion of the UMI library, in the next section, we will give a high-level overview of remote procedure calls and Rust.

\section{Background}
\label{chap3:background}
In distributed computing, a RPC allows a method invocation to be executed on another computer on a shared network. One application of RPC is that in an object-oriented programming paradigm, it enables a method to be invoked on an object stored on a different machine and exchange data across the network. Such a remote method invocation has the same coding as a local invocation, without the programmer explicitly coding the details for the remote interaction.
However, it is hard to support \emph{location transparency}, i.e., in most existing frameworks (e.g., Java RMI), remote invocations do not have \emph{the same semantics} as local invocations. In addition, \emph{memory management} is hard in a distributed setting, for example, distributed garbage collection is complicated.

Rust~\citep{10.5555/3271463} is high-level system programming language which \emph{guarantees memory safety} and \emph{prevents data races} by its \emph{ownership system} for memory management and \emph{borrow checker} for tracking object lifetime of all references in a program during compilation.
Since Rust has semantics that guarantees memory safety, we can extend such guarantees to the distributed computing setting, allowing us to design a RPC framework that provides safe remote method invocations.

\subsection{Remote Procedure Calls}
\label{chap3:background:rpc}
A RPC allows a computer program to request a service from another program located in a different address space, which could be on the same machine or on a different machine across a network. It is a form of client-server communication, where the requesting program is the client, and the service-providing program is the server.

The basic idea behind a RPC system is to make a remote invocation appear like a local invocation, abstracting away the underlying communication mechanisms like message passing and network protocols and simplifying distributed computing by providing a familiar programming model. When a client program calls a procedure, a RPC system will handle the task of transferring the procedure call request to the remote server, along with any necessary parameters or data. The server then executes the requested procedure and sends the results back to the client.

RPCs are particularly useful in distributed systems, where different components of an application are running on separate processes or machines. It allows these components to communicate and share resources efficiently, as if they were part of a single program. There are many common applications of RPCs. For instance, in distributed file systems, RPCs are used in distributed file systems, such as Network File System (NFS), to enable clients to access and manipulate files on remote servers transparently. In remote database access, RPCs facilitate remote access to databases, allowing client applications to execute queries and retrieve data from remote database servers. In designing web services, RPCs form the basis of many web service protocols, such as SOAP (Simple Object Access Protocol), which allows applications to communicate over the internet using XML-based messaging. In modern microservices architectures, RPCs are often used for inter-process communication between different microservices, enabling them to collaborate and share functionality. In object-oriented programming, RPCs are commonly implemented as remote method invocations (RMI), enabling objects on different machines to interact with each other. The core feature of RMIs is that objects can interacted with each other by invoking methods and passing data across the network. This is the application domain that we focus on in this study.

\subsection{Rust}
\label{chap3:background:rust}
As a system programming language emphasising on safety, performance, and concurrency, Rust is designed to prevent some common programming errors, such as data races and dereferencing null pointers. Rust achieves these goals through its unique features of the ownership system, borrow checking, and lifetimes.

In Rust, each value has a variable designated as its \emph{owner}. Each value can only have one owner at a time, and when the owner goes out of scope, the value is dropped, i.e., deallocated from the memory. This ownership model ensures that resources are managed correctly without the need for a garbage collector. The ownership system is fundamental to Rust and serves as the basis for its memory safety.

Rust allows functions and data structures to create references to values without taking ownership. This is called \emph{borrowing}. When a value is borrowed, the original owner cannot modify the value until the borrowing ends. The \emph{borrow checker} is part of Rust's compiler, which ensures that references are used safely and do not result in dangling pointers or other memory issues. Borrowing can be either mutable or immutable, where mutable references have additional constraints to prevent data races and undefined behaviours.

Lifetimes in Rust express how long references should be valid. They assist the borrow checker in ensuring that references do not outlive the data they point to. Rust uses lifetimes to prevent dangling references, which is important for memory safety. Lifetimes are explicitly annotated or inferred. They work alongside the ownership system and borrow checking to maintain memory safety and prevent data races.

With the design of the ownership system, borrow checking mechanism, and lifetimes, Rust enforces strict memory safety guarantees, i.e., means that all references point to valid memory, without requiring a garbage collector. These features also ensure that Rust programmes are free of data race by allowing only one mutable reference at a time or multiple immutable references.

\section{The Implementation of the Rust UMI Library}
\label{chap3:implementation}
In the section, we present our implementation of the UMI framework as a library in Rust. With such a library, a monolithic program can be migrated into a distributed program while preserving the semantics of the original monolithic program.

\subsection{Overview}
\label{chap3:impl:overview}
To give a high-level overview of the design, in figure~\ref{chap3:impl:overview:fig}, we introduce an example of migrating a monolith program into a distributed setting, by adding the \emph{macro}s provided by the UMI library. In this example program which allocates instances of the \texttt{struct} \texttt{A} and calls methods on them, the macro \texttt{\#[proxy\_me]} implicitly translates the declared type \texttt{A} from a \texttt{struct} that can only refer to local resources to an \texttt{enum} that can either hold local resources or be a \emph{proxy} that refers to resources held on a remote node. The initialisation method is translated by the macro \texttt{\#[umi\_init]} to create an instance of the \texttt{enum} \texttt{A} instead of an instance of the \texttt{struct} \texttt{A}. Other methods are also translated by macros to allow both an invocation on a local instance of \texttt{A} and an invocation on a proxy of \texttt{A}. To create a proxy instance, the macro \texttt{remote!(address, ...)} is used, while the syntax of the initialisation of a local instance is unchanged. The invocations of the methods defined for translated \texttt{struct} \texttt{A} take the same form of the invocation those original methods.

An invocation on a proxy is encapsulated into a serialised message and sent to the destination node of which the address is the address stored in the proxy, and then the message is deserialised and the invocation is executed at the destination. After the execution, the result of the invocation is again put into a serialised message and passed back to the calling node to be deserialised.
\begin{figure}[t]
\centering
\begin{subfigure}[t]{0.53\textwidth}
    \centering
\begin{lstlisting}[language=Rust, style=boxed]
#[proxy_me]
struct A { arg: u32 }
impl A {
  #[umi_init]
  new(arg: u32) -> A { A {arg: arg} }
  #[umi_struct_method]
  by_value(&self, a: A) {...}
  #[umi_struct_method]
  by_ref(&self, &a: A) {...}
  #[umi_struct_method]
  by_mut_ref(&self, &mut a: A) {...}
  ...
}
\end{lstlisting}
    % \caption{exmaple}
\end{subfigure}
\hfill
\begin{subfigure}[t]{0.45\textwidth}
    \centering
\begin{lstlisting}[language=Rust, style=boxed]
fn main() {
  let a_remote = 
    remote!\bang!(addr, A::new(10));
  
  let a_local1 = A::new(1);
  let a_local2 = A::new(2);
  let mut a_local3 = A::new(3);
  
  a_remote.by_value(a_local1);
  a_remote.by_ref(&a_local2);
  a_remote
   .by_mut_ref(&mut a_local3);
}
\end{lstlisting}
    % \caption{example}
\end{subfigure}
\vspace{1em}
\caption{Migrating A Monolithic Application into A Distributed Setting with UMI}
\label{chap3:impl:overview:fig}
\end{figure}

\subsection{The Design of the Translation}
\label{chap3:impl:proxy}
As we have seen in the example discussed above, the syntax of a monolithic program is translated into a distributed program by a set of macros. For a declared \texttt{struct}, the macro \texttt{\#[proxy\_me]} performs the translation:
\[
\texttt{struct}\; A\;\{\; \mathit{fields}\;\} \leadsto \texttt{enum}\; A\;\{\; \mathit{Local}\;(\;\mathit{fields}\;), \mathit{Remote}\;(\;\mathit{Address}, \mathit{ID}, \mathit{IsOwner}\;)\;\} 
\]
where the \textit{Address} is the address of the node which stores the resource of a proxy, the \textit{ID} is the identifier of a proxy's resource in the resource table that will be discussed in section~\ref{chap3:impl:resource}, and \textit{IsOwner} denotes whether a proxy is a owned reference or a borrow reference.
This macro can translates an \texttt{enum} to allow it to represent a proxy by adding a new variant which is the proxy:
\[
\texttt{enum}\; A\;\{\; \mathit{variants}\;\} \leadsto \texttt{enum}\; A\;\{\; \mathit{variants}, \mathit{Remote}\;(\;\mathit{Address}, \mathit{ID}, \mathit{IsOwner}\;)\;\} 
\]

The translation of a \texttt{enum} does not affect its initialisation method, however, the translation of a \texttt{struct} requires its initialisation method to be changed accordingly --- instead of creating an instance of a type which is a \texttt{struct}, an instance of a \textit{Local} variant of an \texttt{enum} is created. For instance, in the example shown in figure~\ref{chap3:impl:overview:fig}, the \texttt{new(arg:u32)} method is translated by \texttt{\#[umi\_init]} into:
\begin{lstlisting}[language=Rust, style=boxed, basicstyle=\footnotesize\ttfamily]
new(arg: u32) -> A { A::Local {arg: arg} }
\end{lstlisting}

The macro \texttt{\#[umi\_struct\_method]} performs the translation of other methods of a \texttt{struct}. For instance, the method \texttt{foo1(\&self, a:A)} is translated into:
\begin{lstlisting}[language=Rust, style=boxed, basicstyle=\footnotesize\ttfamily]
fn foo1(&self, a: A) {
  match &self {
    Local(...) => { /* do something */ },
    Remote(...) => { /* remote do something */ }
}}
\end{lstlisting}
Note that within the pattern matching block for the \emph{Remote} variant, the invocation is firstly put into a message and serialised. Then the serialised message is passed to the addressed stored in the proxy, and get deserialised and executed. The result is again put into a message and get serialised. Once it is returned back to the original node, the result is extracted from the deserialised message. Such a communication process between nodes via sending and receiving serialisation/deserialisation messages is completely generated by the macro, freeing programmers from dealing with the message passing complexity.

As for a method of a translated \texttt{enum}, the macro \texttt{\#[umi\_enum\_method]} adds an additional pattern matching block for the proxy variant to the existing pattern matching.

\subsection{Resource Management}
\label{chap3:impl:resource}
To be able to use the UMI library for executing programs that access and manipulate memories of different nodes within a network, resources and computations need to be made available to and well-managed by all nodes in the network.

Firstly, a node should be able to store resources owned by different machines and deallocate those resources according to their lifetime.
In Rust, if some resources are owned by a reference on the same node, and the reference has reached the end of its lifetime, these resources will be deallocated from the memory. With such a design, resources that are not owned by any reference on the same node are automatically deallocated. However, in our UMI library, while some resources on a node $n_1$ is not owned by any reference on the same node, they can be owned by a reference on a different node $n_2$. Although these resources do not have a local owner, the deallocation should not happen until the remote owner reaches the end of its lifetime. 
To achieve this goal, on each UMI server, we design a \emph{resource table} shown in figure~\ref{chap3:impl:tables} on the left, which has the same lifetime of the server. We used it to identify and manage local resources involved in remote computations. Note that the ID in an entry of the table is the ID field in a corresponding proxy.
If a variable is created locally, it will be put into the table once it is passed into a remote computation. The entry will not be removed until the remote computation finishes. If a variable is created via a remote call, it will be put into table on creation and will be deallocated when its remote owner decides that it should be dropped.
\begin{figure}
\centering
\begin{tabular}{ |c|c| } 
\hline
\textbf{ID} & \textbf{Resource} \\\hline
0 & ... \\\hline
1 & ... \\\hline
... & ... \\
\hline
\end{tabular}
\quad
\begin{tabular}{ |c|c| } 
\hline
\textbf{Full Path Name} & \textbf{Type Information} \\\hline
\texttt{A::new} & \texttt{u32}, \texttt{A} \\\hline
\texttt{A::foo1} &  (\texttt{\&A}, \texttt{A}), \texttt{()} \\\hline
... & ... \\
\hline
\end{tabular}
\caption{A resource table (L) and a method registration table (R)}
\label{chap3:impl:tables}
\end{figure}

Secondly, we need to make all nodes aware of all methods that can be invoked on a proxy in order to make computations available on all nodes. To achieve this goal, we register all methods that are available for remote invocations in a \emph{method registration table} shown in figure~\ref{chap3:impl:tables} on the right, using the \texttt{register!(name, arg\_types, return\_type)} macro. The method registration table holds the full path name, argument types, and return type of methods. When a serialised invocation message, which takes the form of a plain string, is received by a node, the method to be invoked is deserialised and reconstructed according to the type information recorded in the registration table.

\subsection{Passing Remote Invocations via Messages}
\label{impl:message}
As briefly discussed in section~\ref{chap3:impl:overview} and section~\ref{chap3:impl:resource}, remote invocations and results of executions are implicitly communicated via serialised and deserialised messages among nodes. We make use of the Serde~\citep{serde} framework to serialise and deserialise these messages and Rust data structures.

There are different types of messages for passing remote invocations. For instance, a remote invocation sent to an receiving node is represented as an invocation message which taking the form of \texttt{Message::Invoke(fname, variables, invoke\_op)}, where \texttt{fname} is the full path name of the method, each variable is annotated with its ownership information (owned or immutably/mutably borrowed), and \texttt{invoke\_op} specifies the ownership information of the return value. The result of the execution of a remote invocation is passed back to the calling node via a return message taking the form of \texttt{Message::Return(return\_var)}, where the \texttt{return\_var} is also annotated with its ownership information. Another important type of messages is the deallocation message which takes the form of \texttt{Message::Drop(id)}, where the \texttt{id} corresponds to an entry key in the resource table shown in figure~\ref{chap3:impl:tables}. Such a message instructs some remotely owned resources to be deallocated.

\subsection{Extending Borrow Checking into Distributed Settings}
\label{chap3:impl:borrow}
To execute a deserialised remote method invocation on the node which receives the invocation, the first step is to gather serialised data as well as the ownership information of each variable involved in the method. In this step, we do not perform any reconstruction of these variables, instead, variables are simply prepared in an appropriate format that can be reconstructed during the execution of the method. Such an format is implemented as \texttt{Argument}, which keeps the information of the variables related to borrow checking, and stores the data of the variable. The detailed implementation of this step is shown in listing~\ref{chap3:impl:lst:invoke}.

As shown in listing~\ref{chap3:impl:lst:invoke}, a serialised variable has a label indicating that if it is a piece of data copied or moved from the caller (\texttt{OwnedLocal}), a remote reference owned by the caller (\texttt{OwnedRemote}), a remote reference immutably borrowed by the caller (\texttt{RefRemote}), or a remote reference mutably borrowed by the caller (\texttt{MutRefRemote}).

\begin{lstlisting}[language=Rust, style=boxed, basicstyle=\footnotesize\ttfamily, caption={Gathering variables from an invocation message}, label=chap3:impl:lst:invoke]
Message::Invoke(fname, variables, invoke_op) => {
  let mut arguments: Vec<Argument> = Vec::new();
  for v in &variables {
    match v {
      Variable::OwnedLocal(s) => 
        { arguments.push(Argument::Serialised(s.clone())); },
      Variable::OwnedRemote(serialise_remote, addr, id) => {
        if addr == &local_address {
          let (owned, is_ref) = mvtable.remove(id).unwrap().into_inner();
          let arg_ref = Argument::Owned(owned);
          arguments.push(arg_ref);
        } else { 
          arguments.push(
            Argument::Serialised(serialise_remote.to_string())); }},
      Variable::RefRemote(serialise_remote, addr, id) => {
        if addr == &local_address {
          let borrow = mvtable.get(id).unwrap().borrow();
          let ptr: *const (Box<dyn Any + Send + Sync>, bool) = &*borrow;
          unsafe {
            let back: &(Box<dyn Any + Send + Sync>, bool) 
              = ptr.as_ref().unwrap();
            let arg_ref = Argument::Ref(&back.0, back.1);
            arguments.push(arg_ref); }
        } else { 
          arguments.push(
            Argument::RemoteRef(serialise_remote.to_string())); }},
      Variable::MutRefRemote(serialise_remote, addr, id) => {
        if addr == &local_address {
          let mut borrow_mut = mvtable.get(id).unwrap().borrow_mut();
          let ptr: *mut (Box<dyn Any + Send + Sync>, bool)
            = &mut *borrow_mut;
          unsafe {
            let back: &mut (Box<dyn Any + Send + Sync>, bool) 
              = ptr.as_mut().unwrap();
            let arg_ref = Argument::MutRef(&mut back.0, back.1);
            arguments.push(arg_ref); }
        } else { 
          arguments.push(
            Argument::RemoteMutRef(serialise_remote.to_string())); }
  }}} ...
}
\end{lstlisting}

If a variable is serialised data, which is copied or moved from the caller, it will be kept as serialised, since the deserialisation and reconstruction process will happen during the invocation of the method (line 5 --- line 6).
If a variable is a proxy which is located at the receiver, then it will be obtained from the resource table shown in figure~\ref{chap3:impl:tables}. According to its ownership information, if it is moved, then the corresponding entry will be removed from the resource table (line 7 --- line 11). 
If it is immutably borrowed, then the corresponding entry will be immutably borrowed from the table (line 15 --- line 23). If it is mutably borrowed, then the corresponding entry will be mutably borrowed from the table and updated after the execution (line 27 --- line 36). 
If an argument is a proxy that is not located at the caller, as shown in three \texttt{else}-cases, from line 12 to line 14, from line 24 to line 26, and from line 37 to line 39, then the proxy will be passed into the method without any additional modification.

Once the information of all variables are gathered and processed, the invocation will be executed and the result will then be sent back to the caller. The implementation of the execution of this invocation is shown in listing~\ref{chap3:impl:lst:exec-return}. The method information, mainly ownership and type information of the arguments, and return value of a method are retrieved from the registration table shown in figure~\ref{chap3:impl:tables} on the right. During the execution of the method via \texttt{f.call(arguments)} shown in line 6, serialised arguments and boxed argument entries retrieved from the resource table are reconstructed according to the registered type information.

The result of an execution is provided in two formats, serialised data and a boxed data entry. These two formats are used according to the required ownership information of the return value. 
If the method produces an owned result annotated with \texttt{InvokeOp::Owned}, no matter the serialised data \texttt{res} represents some local resources or a proxy, it will be kept as the serialised form and sent back via a return message (line 9 --- line 11). 

If the method is an initialisation call sent by the macro \texttt{remote!(...)} annotated with \texttt{InvokeOp::Init}, the boxed entry data will be inserted into the resource table and an unique \texttt{id} will be generated. In the return message, the address of the receiver, the \texttt{id}, and the ownership status which is \texttt{true} are included for the caller to construct a proxy that owns such a data entry on the receiver (line 12 --- line 18).

For a return value that is an immutably or mutably borrowed reference, there are two situations. 
If the borrowed reference is local to the receiver, the reference itself will be inserted into the resource table identified by an generated unique \texttt{id}. Such an \texttt{id} and the address of the receiver will be sent back to the caller for creating an proxy that mirrors this borrowed reference (line 20 --- line 25 and line 31 --- line 36). 
However, if a borrowed reference is not local to the receiver, meaning it already mirrors a reference on a different node, then it will not be stored in the resource table, instead, the serialised version of it will be sent back to the caller in a return message (line 26 --- line 27 and line 37 --- line 38).
\begin{lstlisting}[language=Rust, style=boxed, basicstyle=\footnotesize\ttfamily, caption={Executing an invocation and returning the result}, label=chap3:impl:lst:exec-return]
Message::Invoke(fname, variables, invoke_op) => {
  ...
  let f: &str = &*fname; 
  match lrtable.get(f) {
    Some(f) => {
      let ((res, is_local), b) = f.call(arguments);
      let res_message: Message;
      match invoke_op {
        InvokeOp::Owned => { 
          res_message = Message::Return(ReturnVar::Owned(res)); 
        },
        InvokeOp::Init => {
          let id = (SystemTime::now(), m_id_gen.next());
          // b is the resource
          mvtable.insert(id.clone(), RefCell::new((b, false)));
          res_message = 
            Message::Return(ReturnVar::OwnedInit(local_address, id, true));
        },
        InvokeOp::Ref => {
          if is_local {
            let id = (SystemTime::now(), m_id_gen.next());
            // b is a reference
            mvtable.insert(id.clone(), RefCell::new((b, true)));
            res_message =
              Message::Return(ReturnVar::RefMirror(local_address, id));
          } else { 
            res_message = Message::Return(ReturnVar::RefBorrow(res)); 
          }
        },
        InvokeOp::MutRef => {
          if is_local {
            let id = (SystemTime::now(), m_id_gen.next());
            // b is a reference
            mvtable.insert(id.clone(), RefCell::new((b, true)));
            res_message =
              Message::Return(ReturnVar::MutRefMirror(local_address, id));
          } else { 
            res_message = Message::Return(ReturnVar::MutRefBorrow(res)); 
          }
      }}
      response(stream, res_message);
    },
    None => { /* report unregistered function */ }}
}, ...
\end{lstlisting}

\subsection{Extending Lifetime Management into Distributed Settings}
\label{chap3:impl:lifetime}
Recall that in section~\ref{chap3:impl:resource}, we have briefly introduced storing and deallocating remotely owned resources on a node. In this section we discuss the design and implementation of a remote deallocation in detail. 

In a monolithic Rust program, when a variable that owns some resources reaches the end of its lifetime, in most cases, out of a program's scope, the resources it owns will be automatically deallocated. We extend this feature to the distributed setting. As illustrated in listing~\ref{chap3:impl:lst:drop-eg}, when the given proxy \texttt{a\_proxy} is initialised, some resources are allocated to the receiver node with the address \texttt{addr} (line 7). Although these resources do not have an owner on the same node, they should not be deallocated until its remote owner \texttt{a\_proxy} reaches the end of its lifetime (line 10). 
\begin{lstlisting}[language=Rust, style=boxed, basicstyle=\footnotesize\ttfamily, caption={An example of a remote deallocation}, label=chap3:impl:lst:drop-eg]
// on caller
fn main() {
  ...
  // the data of a_proxy is in the table on the receiver with addr
  // but it is owned by the caller and will be deallocated
  // when its owner decides to drop it
  let a_proxy = remote!\bang!(addr, A::new(10)); 
  ...
  a_proxy.foo1(...)
} // a_proxy is out of scope, its data on the remote machine is dropped
\end{lstlisting}

For monolithic programs, the deallocation is achieved via the \texttt{drop} method in the destructor trait \texttt{Drop}, which in most cases is automatically implemented for Rust types. We extend this \texttt{drop} method to handle the deallocation of remotely owned resources. The implementation is shown in listing~\ref{chap3:impl:lst:drop}.
When a proxy that owns some resources on a node reaches the end of its lifetime, a serialised deallocation message is automatically sent to the node that holds these resources. 
\begin{lstlisting}[language=Rust, style=boxed, basicstyle=\footnotesize\ttfamily, caption={The implementation of a remote deallocation}, label=chap3:impl:lst:drop]
...
impl Drop for #name {
  fn drop(&mut self) {
    match self {
      Self::Remote(addr, id, is_owner) => {
        if is_owner.load(Ordering::Relaxed) {
          let msg = Message::Drop(*id);
          send(*addr, msg).unwrap(); }},
      _ => {}
}}}
...
\end{lstlisting}
As shown below, once a deallocation message is received by the targeted receiver, the entry with the corresponding \texttt{id} will be removed from the resource table (i.e., the \texttt{mvtable} in the listing). 
\begin{lstlisting}[language=Rust, style=boxed, basicstyle=\footnotesize\ttfamily]
Message::Drop(id) => { mvtable.remove(&id); }
\end{lstlisting}

After presenting the design and implementation of the UMI library, in the next chapter, we discuss the formalisation of core concepts of the UMI library base on formalised operational semantics of a core language of Rust, and sketch a proof of the location transparency theorem stating that using the UMI framework, the translation of a monolithic program into a distributed program preserves the semantics of the original program.

\section{The Operational Semantics}
\label{chap3:semantics}
We first provide a small-step operational semantics of a core language of Rust based on \citet{10.1145/3443420}'s featherweight Rust (FR), which captures the core features of Rust including copy- and move-semantics, owned and immutably/mutably borrowed references, and lexical lifetimes. We then present dFR, which extends FR to include distributed features of the UMI framework such as remote copy- and move-semantics as well as remote references. We intend to show that such an extension preserves the semantics of FR, and therefore, the type safety claims of FR is preserved by dFR.

\subsection{The Revised Syntax and Semantics of FR}
\label{chap3:semantics:fr}
We present a revised syntax of FR in figure~\ref{chap3:syntax:r-syntax-fig}. Note that $\&w$ and $\&\texttt{mut}\;w$ represent immutable and mutable borrowing, $\mathscr{l}^\bullet$ and $\mathscr{l}^\circ$ are owned and borrowed references where $\mathscr{l}$ represents a location in a program state. In addition, different from the original FR and Rust which do not have explicit syntax for move, we use $\#w$ to express move explicitly.

% Note that the $k$ in the block represents the lifetime.
\begin{figure}
\begin{alignat*}{3}
    \text{Term} \quad t \ \metaDeff \quad &\texttt{let mut}\;x = t;t &\text{declaration}\\
    \cmid \quad &w \metaDef t &\text{assignment}\\
    \cmid \quad &t;t &\text{sequence}\\
    \cmid \quad &\texttt{()} &\text{unit}\\
    \cmid \quad &\{t\} &\text{block}\\
    \cmid \quad &\texttt{box}\;t &\text{heap allocation}\\
    \cmid \quad &\&w \quad &\text{immutable borrow} \\
    \cmid \quad &\&\texttt{mut}\;w \quad &\text{mutable borrow} \\
    \cmid \quad &\#{w} \quad &\text{move} \\
    \cmid \quad &!{w} \quad &\text{copy} \\
    \cmid \quad &v \quad &\text{value} \\
    % \cmid \quad &w\\
    \text{LVal} \quad w \ \metaDeff \quad &x \quad &\text{variable} \\
    \cmid \quad &{*}w \quad &\text{dereference} \\
    \text{Value}(\mathcal{V}) \quad v \ \metaDeff \quad &\bot\\ 
    \cmid \quad &\texttt{()} \quad &\text{unit} \\
    \cmid \quad &i \quad &\text{integer} \\
    \cmid \quad &\mathscr{l}^\bullet &\text{owned reference} \\
    \cmid \quad &\mathscr{l}^\circ &\text{borrowed reference} \\
    \text{Location}\quad  \mathscr{l} \in \mathbb{A}
\end{alignat*}
\caption{The revised syntax of FR}
\label{chap3:syntax:r-syntax-fig}
\end{figure}

\begin{figure}
\begin{align*}
    &\mathcal{S}: \mathbb{A} \rightharpoonup \mathcal{V} \times \mathcal{L}\\
    &\mathcal{S} \mid \mathscr{l} \mapsto (v, m) \quad \text{where: } \mathscr{l} \in \textbf{dom}\;\mathcal{S} &\text{(update)}\\
    &\mathcal{S} \otimes \mathscr{l} \mapsto (v, m) \quad \text{where: } \mathscr{l} \notin \textbf{dom}\;\mathcal{S} &\text{(extend)}
\end{align*}
\caption{The program state}
\label{chap3:semantics:program:state}
\end{figure}

The notion of program state is introduced in figure~\ref{chap3:semantics:program:state}, which is a mapping from a location to a tuple of value and lifetime. When a value is replaced or its lifetime is expired, it will be removed from the program state.

We provide the operations of recursively removing values based on a location $\mathscr{l}$ and a lifetime $k$ from the state: 
\begin{align*}
      &\bigl(\mathcal{S}\otimes(\mathscr{l}^\bullet \mapsto (v, k))\bigr)\setminus {\mathscr{l}^\bullet} = \mathcal{S}\setminus v \\
      &\mathcal{S}\setminus v = \mathcal{S} \quad\text{(o/w)}
\end{align*}
and respectively:
\[
   \mathcal{S}\setminus k \ (\mathscr{l}) =
    \begin{cases}
      \mathcal{S}(\mathscr{l}) \quad\text{if $\mathcal{S}(\mathscr{l})=(v,k)$} \\
      \textbf{undefined}\quad\text{(o/w)}
    \end{cases}
\] % this  is not correct and I will revise it

Figure~\ref{semantics:r-reduction-fig} shows the small-step operational semantics of FR, which takes the form of a reduction $S, t \longrightarrow S', t'$, where $S$ is the program state before the evaluation of the term $t$, and $S', t'$ are the program state and the term after the evaluation.

Evaluating a copy term simply makes a copy of a value $v$ at a given location $\mathscr{l}$, without modifying the program state, whereas evaluating a move term moves a value $v$ out of a given location $\mathscr{l}$.
A heap allocation $\texttt{box}\;v$ puts the value $v$ into a fresh location $\mathscr{l}$ and gives it the global lifetime $\top$, which outlives all other lifetimes. The rule for evaluating a borrow term produces a borrowed reference of the give location $\mathscr{l}$.
Assignment places a given value $v'$ in the location $\mathscr{l}$, and recursively deallocates the old value $v$ from the program state. Note that assignments to immutably borrowed references are prohibited by the type system provided by \citepos{10.1145/3443420} original work, the discussion of the type system is omitted here since focus on the analysis of the SOS of FR programs.
The evaluation of a declaration allocates a given value $v$ to a fresh location $\mathscr{l}$ and substitutes latter occurrence of the declared variable $x$ with the owned reference $\mathscr{l}^\bullet$.

FR's lifetimes are based on the the lexical structure of programs. A block's lifetime $k$ is based on the depth of the block. A block with deeper depth $\mathit{suc}\; k$ lives shorter than a block with depth $k$. The evaluation of a block is the evaluation of the term inside the block. At the end of the evaluation, a single value $v$ is obtained and values that have short lifetime than the current block are deallocated from the program state. The reduction rules for the evaluation of sequences are intuitive. We highlight the case for a sequence of which the first term is an owned reference. After evaluating the owned reference, it is recursively deallocated from the program state.
\begin{figure}
\begin{mathparpagebreakable}
    \inferrule*[right={(Copy)}]{\mathcal{S} (\mathscr{l}) = (v , m)}
    {\mathcal{S}, !{\mathscr{l}^\bullet} \longrightarrow \mathcal{S}, v}
    
    \inferrule*[right={(Move)}]{ }
    {\mathcal{S}\otimes \mathscr{l} \mapsto (v, m), \#\mathscr{l}^\bullet \longrightarrow \mathcal{S}\otimes \mathscr{l} \mapsto \bot, v}

    \inferrule*[right={(Box)}]{\mathscr{l} \notin \textbf{dom}\;\mathcal{S}}
    {\mathcal{S}, \texttt{box }v \longrightarrow \mathcal{S}\otimes \mathscr{l}\mapsto (v,\top), \mathscr{l}^\bullet}
    % \vspace{-0.3cm}
    % \\\text{Heap locations are given global lifetime $\top$ which all other lifetimes are assumes to be inside.}

    \inferrule*[right={(Borrow)}]{\mathscr{l} \in \textbf{dom}\; \mathcal{S}}
    {\mathcal{S}, \&[\texttt{mut}]\mathscr{l}^\bullet \longrightarrow \mathcal{S}, \mathscr{l}^\circ}

    \inferrule*[right={(Assign Owned)}]{ }
    {\mathcal{S}\otimes \mathscr{l} \mapsto (v, m), \mathscr{l}^\bullet \metaDef v' \longrightarrow (\mathcal{S}\setminus v)\otimes \mathscr{l} \mapsto (v', m), \texttt{()}}
    % \\\text{Assignment should not change the lifetime of the original location}

    \inferrule*[right={(Assign Borrowed)}]{ }
    {\mathcal{S}\otimes \mathscr{l} \mapsto (v, m), \mathscr{l}^\circ \metaDef v' \longrightarrow (\mathcal{S}\setminus v)\otimes \mathscr{l} \mapsto (v', m), \texttt{()}}

    \inferrule*[right={(Decl)}]{\mathscr{l} \notin \textbf{dom}\;\mathcal{S}}
    {\mathcal{S}, \texttt{let}\;\texttt{mut}\;x = v; t \stackrel k \longrightarrow \mathcal{S}\otimes \mathscr{l} \mapsto (v, k), t[x/\mathscr{l}^\bullet]}\\

    \inferrule*[right={(Block (base))}]{ }
    {\mathcal{S}, \{v\} \stackrel k\longrightarrow \mathcal{S}\setminus suc\ k, v} 
    
    \inferrule*[right={(Block (suc))}]
    {\mathcal{S}, t \stackrel {suc\ k}\longrightarrow \mathcal{S'}, t'}
    {\mathcal{S}, \{t\} \stackrel k\longrightarrow \mathcal{S'}, \{t'\}}
    
    \inferrule*[right={(Seq-OwnedRef)}]{\mathscr{l} \in \textbf{dom}\; \mathcal{S}}
    {\mathcal{S}, \mathscr{l}^\bullet; t \longrightarrow \mathcal{S}\setminus\mathscr{l}^\bullet, t} 
    
    \inferrule*[right={(Seq-BorrowedRef)}]{\mathscr{l} \in \textbf{dom}\; \mathcal{S}}
    {\mathcal{S}, \mathscr{l}^\circ; t \longrightarrow \mathcal{S}, t}

    \inferrule*[right={(Seq-Int)}]{ }
    {\mathcal{S}, i; t \longrightarrow \mathcal{S}, t}
    
    \inferrule*[right={(Seq-Unit)}]{ }
    {\mathcal{S}, \texttt{()}; t \longrightarrow \mathcal{S}, t}
\end{mathparpagebreakable}
    \caption{The semantics of revised FR}
    \label{semantics:r-reduction-fig}
\end{figure}
% \subsection{Evaluation Context}

An evaluation context is a term with a placeholder $\llbracket\cdot\rrbracket$. $E\llbracket t\rrbracket$ is a term obtained by replacing the placeholder with a term $t$. The evaluation context and reduction rule for the evaluation context are shown in figure~\ref{semantics:eval-context}.
\begin{figure}
    \begin{align*}
        E \metaDeff \llbracket \cdot \rrbracket \cmid E;t \cmid v; E \cmid \texttt{let mut }x=E; t \cmid \texttt{let mut }x = v; E \cmid \{E\} \cmid \texttt{box }E \cmid w = E
    \end{align*}
    \begin{mathpar}
    \inferrule*[right={(Context)}]{ S, t \longrightarrow S, t' }
        { S, E\llbracket t \rrbracket \longrightarrow  S', E\llbracket t' \rrbracket }
    \end{mathpar}
    \caption{Evaluation context}
    \label{semantics:eval-context}
\end{figure}

Next, we will discuss the syntax and semantics of dFR, which extends FR with distributed computation features including remote references and values.

\subsection{The Syntax and Semantics of dFR} 
\label{chap3:semantics:dfr}
Figure~\ref{syntax:d-syntax-fig} shows the syntax of dFR, which is the syntax of FR discussed in section~\ref{chap3:semantics:fr} extended with the remote declaration $\texttt{let}\;\texttt{mut}@n\; x = t;t$, remote heap allocation $\texttt{box}\;t$, remote values $v@n$, and remote terms $t@n$. Note that $n$ is the address of a node and $\mathcal{N}$ is the set of addresses. Note that such an extension does not change the type system of FR.
% $\mathscr{l}^\bullet_{@n}$ denotes a owned reference on the node $n$. 
\begin{figure}
\begin{alignat*}{3}
    \text{Term}(\mathcal{T}) \quad t \ \metaDeff \quad &\texttt{let mut } x = t;t &\text{declaration}\\
    \cmid \quad &\texttt{let mut}@n\; x = t;t &\text{remote declaration}\\
    \cmid \quad &w \metaDef t &\text{assignment}\\
    \cmid \quad &t;t &\text{sequence}\\
    \cmid \quad &\texttt{()} &\text{unit}\\
    \cmid \quad &\{t\} &\text{block}\\
    \cmid \quad &\texttt{box } t &\text{heap allocation}\\
    \cmid \quad &\texttt{box}@n\; t &\text{remote  heap allocation}\\
    \cmid \quad &\&w \quad &\text{immutable borrow} \\
    \cmid \quad &\&\texttt{mut } w \quad &\text{mutable borrow} \\
    \cmid \quad &\#{w} \quad &\text{move} \\
    \cmid \quad &!{w} \quad &\text{copy} \\
    \cmid \quad &v \quad &\text{value} \\
    \cmid \quad &v@n \quad &\text{remote value}\\
    % \cmid \quad &t@n \quad &\text{remote term}\\
    \text{Remote Term} \quad t_d \metaDeff \quad &t@n\\
    \text{LVal} \quad w \ \metaDeff \quad &x \quad &\text{variable} \\
    \cmid \quad &{*}w \quad &\text{dereference} \\
    \text{Value}(\mathcal{V}) \quad v \ \metaDeff \quad &\bot\\ 
    \cmid \quad &\texttt{()} \quad &\text{unit} \\
    \cmid \quad &i \quad &\text{integer} \\
    \cmid \quad &\mathscr{l}^\bullet &\text{owned reference} \\
    \cmid \quad &\mathscr{l}^\circ &\text{borrowed reference} \\
    \text{Location}\quad  \mathscr{l} \in \mathbb{A}
    &\quad\quad\quad\quad\text{Node}\quad n \in \mathcal{N}
\end{alignat*}
\caption{The syntax of dFR}
\label{syntax:d-syntax-fig}
\end{figure}

Building upon the program state $\mathcal{S}$ for FR, in figure~\ref{d-state}, we introduce the distributed program state $\mathcal{D}$, which maps addresses of nodes to their program states. The reduction rule takes the form of $\mathcal{D}, \mathcal{C} \longrightarrow \mathcal{D'}, \mathcal{C}'$, where $\mathcal{C}$ and $\mathcal{C}'$ are configuration stacks. For each reduction, the term on the top of a configuration stack $\mathcal{C}$ gets evaluated. The element of the configuration stack can either be a pair of an address and a hole ($n, ?$) or a pair of an address and a term ($n, t$). Utilising the distributed program state and the configuration stack, we provide the semantics of dFR in figure~\ref{semantics:eval-distributed-1} and figure~\ref{semantics:eval-distributed-2}.
\begin{figure}
    \begin{align*}
        &\mathcal{D}: \mathcal{N} \rightharpoonup \mathcal{S}
        %&\mathit{nt} \in  \mathcal{N} \times\mathds{1} + \mathcal{N}\times T\\
        &\mathit{nt} \in  \mathcal{N} \times\mathds{1} + \mathcal{N}\times T\\
        &\mathcal{C} : (nt)^*
        &\mathrm{Configuration}: \mathcal{D}, \mathcal{C}
    \end{align*}
    \caption{Distributed program state and configuration stack}
    \label{d-state}
\end{figure}

\begin{figure}
    \begin{mathpar}
        \inferrule*[right={(Copy (s1))}]{ \mathcal{D}(n')(\mathscr{l}) = (v, m) }
        {\mathcal{D}, \mathcal{C} \concat (n, !\mathscr{l}^\bullet_{@n'}) \longrightarrow \mathcal{D}, \mathcal{C} \concat (n, ?) \concat (n', !\mathscr{l}^\bullet)}
        
        \inferrule*[right={(Copy (s2))}]{ \mathcal{D}(n')(\mathscr{l}) = (v, m)}
        {\mathcal{D}, \mathcal{C} \concat (n, ?) \concat (n',!\mathscr{l}^\bullet) \longrightarrow \mathcal{D}, \mathcal{C} \concat (n, v@n')}

        \inferrule*[right={(Move (s1))}]{ }
        {\mathcal{D} \otimes (n' \mapsto \mathcal{S}\otimes\mathscr{l} \mapsto (v, m)), \mathcal{C} \concat (n, \#\mathscr{l}^\bullet_{@n'}) \longrightarrow \\\mathcal{D} \otimes (n' \mapsto \mathcal{S}\otimes\mathscr{l} \mapsto (v, m)), \mathcal{C} \concat (n, ?) \concat (n', \#\mathscr{l}^\bullet)}

        \inferrule*[right={(Move (s2))}]{ }
        {\mathcal{D} \otimes (n' \mapsto \mathcal{S}\otimes \mathscr{l} \mapsto (v, m)), \mathcal{C} \concat (n, ?) \concat (n', \#\mathscr{l}^\bullet) \longrightarrow \\\mathcal{D} \otimes (n' \mapsto \mathcal{S}\otimes\mathscr{l} \mapsto \bot), \mathcal{C} \concat (n, v@n')}

        \inferrule*[right={(Box (s1))}]{ }
        {\mathcal{D}, \mathcal{C} \concat (n, (\texttt{box}@n'\;v)) \longrightarrow \mathcal{D}, \mathcal{C} \concat (n, ?) \concat (n', \texttt{box}\;v)}

        \inferrule*[right={(Box (s2))}]{ \mathscr{l} \notin \textbf{dom}\;\mathcal{D}(n') }
        {\mathcal{D}, \mathcal{C} \concat (n, ?) \concat (n', \texttt{box}\;v) \longrightarrow \mathcal{D} \mid (n' \mapsto \mathcal{D}(n')\otimes\mathscr{l}\mapsto (v, \top)), \mathcal{C} \concat (n, \mathscr{l}^\bullet_{@n'})} 

        \inferrule*[right={(Borrow (s1))}]{ \mathscr{l} \in \textbf{dom}\;\mathcal{D}(n') }
        {\mathcal{D}, \mathcal{C} \concat (n, \&[\texttt{mut}]\mathscr{l}^\bullet_{@n'}) \longrightarrow \mathcal{D}, \mathcal{C} \concat (n, ?) \concat (n', \&[\texttt{mut}]\mathscr{l}^\bullet)}

        \inferrule*[right={(Borrow (s2))}]{ \mathscr{l} \in \textbf{dom}\;\mathcal{D}(n') }
        {\mathcal{D}, \mathcal{C} \concat (n, ?) \concat (n', \&[\texttt{mut}]\mathscr{l}^\bullet) \longrightarrow \mathcal{D}, \mathcal{C} \concat (n, \mathscr{l}^\circ_{@n'})}
    \end{mathpar}
    \caption{The semantics of dFR (part one)}
    \label{semantics:eval-distributed-1}
\end{figure}

To evaluate copying a remotely owned reference $\mathscr{l}^\bullet_{@n'}$ on a node with address $n$, the first step shown in the rule \textsc{Copy (s1)} is to update the configuration stack by changing the $(n, !\mathscr{l}^\bullet_{@n'})$ to $(n, ?)$, and pushing a new address-term pair $(n', !\mathscr{l}^\bullet)$ to be evaluated onto the stack. It models that the computation is passed to the node $n'$, which stores the resource owned by the reference $\mathscr{l}^\bullet_{@n'}$. Then the rule \textsc{Copy (s2)} indicates that the copy term $!\mathscr{l}^\bullet$ gets evaluated on the node $n'$, and the resulting value annotated with the address $n'$ is passed back to fill in the hole. Similarly, as for the semantics of moving the value out of a remote owned reference, the first step is to replace the term on the node $n$ with a hole, and pass the copy term to the node $n'$ to evaluate. The resulting value $v$ annotated with the address $n'$ is passed back to the node $n$ at the end of the evaluation to fill in the hole, and the value of the location $\mathscr{l}$ at the program state of the node $n'$ is replaced by $\bot$, indicating that the value is moved out of the location $\mathscr{l}$ at the node $n'$.

\begin{figure}
    \begin{mathpar}
        \inferrule*[right={(Assign Owned (s1))}]{ }
        {\mathcal{D} \otimes (n' \mapsto \mathcal{S}\otimes \mathscr{l} \mapsto (v, m)), \mathcal{C} \concat (n, \mathscr{l}_{@n'}^\bullet \metaDef v') \longrightarrow \\ \mathcal{D}\otimes (n' \mapsto \mathcal{S}\otimes \mathscr{l} \mapsto (v, m)), \mathcal{C} \concat (n, ?) \concat (n', \mathscr{l}^\bullet \metaDef v')}

        \inferrule*[right={(Assign Owned (s2))}]{ }
        {\mathcal{D}\otimes (n' \mapsto \mathcal{S}\otimes \mathscr{l} \mapsto (v, m)), \mathcal{C} \concat (n, ?) \concat (n', \mathscr{l}^\bullet \metaDef v') \longrightarrow \\ \mathcal{D}\otimes (n' \mapsto \mathcal{S} \setminus v \otimes \mathscr{l} \mapsto (v', m)), \mathcal{C} \concat (n, \texttt{()})}

        \inferrule*[right={(Assign Borrowed (s1))}]{ }
        {\mathcal{D} \otimes (n' \mapsto \mathcal{S}\otimes \mathscr{l} \mapsto (v, m)), \mathcal{C} \concat (n, \mathscr{l}_{@n'}^\circ \metaDef v') \longrightarrow \\ \mathcal{D}\otimes (n' \mapsto \mathcal{S}\otimes \mathscr{l} \mapsto (v, m)), \mathcal{C} \concat (n, ?) \concat (n', \mathscr{l}^\circ \metaDef v')}

        \inferrule*[right={(Assign Borrowed (s2))}]{ }
        {\mathcal{D}\otimes (n' \mapsto \mathcal{S}\otimes \mathscr{l} \mapsto (v, m)), \mathcal{C} \concat (n, ?) \concat (n', \mathscr{l}^\circ \metaDef v') \longrightarrow \\ \mathcal{D}\otimes (n' \mapsto \mathcal{S} \setminus v \otimes \mathscr{l} \mapsto (v', m)), \mathcal{C} \concat (n, \texttt{()})}
        
        \inferrule*[right={(Decl (s1))}]{ }
        {\mathcal{D}, \mathcal{C} \concat (n, \texttt{let mut}@n'\; x = v; t) \longrightarrow\\ \mathcal{D}, \mathcal{C} \concat (n, t[x/?]) \concat (n', \texttt{let mut } x = v; x)}

        \inferrule*[right={(Decl (s2))}]{ \mathscr{l} \notin \textbf{dom}\;\mathcal{D}(n') }
        {\mathcal{D}, \mathcal{C} \concat (n, t[x/?]) \concat (n', \texttt{let mut } x = v; x) \longrightarrow \\\mathcal{D} \otimes (n' \mapsto \mathcal{S}\otimes \mathscr{l} \mapsto (v, k)), \mathcal{C} \concat (n, t[x/ \mathscr{l}^\bullet_{@n'}])}

        \inferrule*[right={(Local Terms)}]{ \mathcal{S}, t \longrightarrow \mathcal{S}', t'}
        {\mathcal{D}\otimes(n \mapsto \mathcal{S}), \mathcal{C} \concat (n, t) \longrightarrow \mathcal{D}\otimes(n \mapsto \mathcal{S}'), \mathcal{C} \concat (n, t')}

        \inferrule*[right={(Remote Terms)}]{\mathcal{D}\otimes(n' \mapsto \mathcal{S}), \mathcal{C} \concat (n', t) \longrightarrow \mathcal{D}\otimes(n' \mapsto \mathcal{S}'), \mathcal{C} \concat (n', t')}
        {\mathcal{D}\otimes(n' \mapsto \mathcal{S}), \mathcal{C} \concat (n, t@n') \longrightarrow \mathcal{D}\otimes(n' \mapsto \mathcal{S}'), \mathcal{C} \concat (n, t'@n')}
    \end{mathpar}
    \caption{The semantics of dFR (part two)}
    \label{semantics:eval-distributed-2}
\end{figure}

The rules \textsc{Box (s1)} and \textsc{Box (s2)} show the evaluation of a remote heap allocation $\texttt{box}@n'\;v$ on the node $n$. Firstly, the heap allocation is passed to the node $n'$ to be evaluated, and a hole on the node $n$ is created and pushed onto the configuration stack. The value $v$ is stored in a fresh location $\mathscr{l}$ at the program state of the node $n'$ and assigned with the lifetime $\top$ since it is a heap allocation, hence a owned reference $\mathscr{l}^\bullet$ is created in the node $n'$. Such a owned reference is then passed back to the node $n$ allowing the node $n$ to own the location $\mathscr{l}$ created by the heap allocation on the node $n'$. At the end of the evaluation, as shown in the rule \textsc{Box (s2)}, on the top of the configuration stack, the hole create on the node $n$ is filled by the owned remote reference $\mathscr{l}^\bullet_{@n'}$

The rules \textsc{Borrow (s1)} and \textsc{Borrow (s2)} show the evaluation of immutable and mutable borrow terms. On a given node $n$, to borrow a remotely owned reference from a different node $n'$, firstly a hole is again created and pushed waiting for a term to be passed back, and the borrow term $\&[\texttt{mut}]\mathscr{l}^\bullet_{@n'}$ is passed to the node $n'$ to be evaluated. After it being evaluated on the node $n'$, the resulting remotely borrowed reference $\mathscr{l}^\circ_{@n'}$ is passed back to the node $n$ filling the hole on the configuration stack.

A remote assignment to an owned reference $\mathscr{l}^\bullet_{@n'} \metaDef v'$ on the node $n$ assigns a new value $v'$ to its remotely owned reference on the node $n'$, which is shown in the rule \textsc{Assign Owned (s1)} and \textsc{Assign Owned (s2)}. The first step is again leaving a hole awaiting to be filled on the configuration stack and passing the assignment to the node $n'$ to be evaluated. In the next step, the evaluation of the assignment on the node $n'$ modifies the program state on $n'$ by recursively deallocates the old value $v$ which is stored in the location $\mathscr{l}$. And then the location $\mathscr{l}$ on the node $n'$ is assigned with the new value $v'$. Since the assignment produces only a unit value $\texttt{()}$, it will be passed back and fill in the hole on the configuration stack. The evaluation of a remote assignment to a borrowed reference is similar. Note that again, since dFR extends FR without any modification of FR's type system, same to assignments in FR, remote assignments to immutable references in dFR are also prohibited by the type system.

Demonstrated in \textsc{Decl (s1)} and \textsc{Decl (s2)}, the evaluation of a remote declaration is more complicated. The first step shown in \textsc{Decl (s1)} leaves a hole to be filled by the resulting term in the substitution of the declared variable $x$. Then the declaration is passed to the node $n'$ to be evaluated. Shown in  \textsc{Decl (s2)}, once a fresh location $\mathscr{l}$ on the node $n'$ is allocated with the value $v$, the owned reference $\mathscr{l}^\bullet$ will then be passed back to the node $n$ and all occurrences of the declared variable $x$ on the node $n$ will be substituted with the remote owned reference $\mathscr{l}_{@n'}^\bullet$.

Lastly, all reduction rules for evaluating terms presented in FR are adapted into reduction rules for evaluating dFR via the rule \textsc{Local Term}. As for the evaluation of remote terms, shown in the rule \textsc{Remote Term}, if a local term $t$ on the node $n'$ is evaluated into $t'$, then when it is treated as a remote term $t@n'$ on the node $n$, it will be evaluated to a remote term $t'@n'$ on the node $n$.

In the next section, we present and prove a location transparency theorem, which states that when translating a monolithic program written in FR into a distributed program written in dFR, the semantics of the monolithic program is preserved.

\subsection{Preservation of Semantics when Translating a FR Program into a dFR Program}
\label{chap3:semantics:loc-transp}
As we have mentioned in previous sections, by extending FR into dFR, the type system and static borrow checking of the validity of owning, immutably borrowing and mutably borrowing resources remain unchanged. By formalising the semantics of FR and dFR, we would like to show that, when we flatten a distributed program in dFR into a monolithic program in FR, the flattened result of the execution of the distributed program should be the same as the result of the execution of the flattened single node program. By demonstrating that the distributed program preserves the original semantics of the monolithic program, we can then conclude that the memory safety guarantees provided by FR's type system and static checking can be extended into distributed program in dFR.

Formally, we state this semantic preservation property of the distributed extension dFR in the \emph{location transparency theorem}~\ref{chap3:thm:loc-transp}. Note that the reverse direction of the theorem does not hold. Since to extend a given single node program into a distributed program by allocating the program state on arbitrary nodes and making the term involved in the execution containing remote components that live on arbitrary nodes may not lead to constructing a distributed program that can be successfully executed. However, because our goal is to show that the distributed program preserves the same behaviour as if it is a single node program, having only one direction in the theorem is sufficient.

\begin{theorem}[Location Transparency]
\emph{For any term $t$, given an initial distributed program state $\mathcal{D}$ and an initial single node program state $\mathcal{S}$, where the flattened distributed program state equals to the single node program state, if a distributed execution of a term $t$ that may be a remote term or contain remote component with the distributed program state $\mathcal{D}$ results in a distributed program state $\mathcal{D}'$ and a value $v$ which can be either remote or local, then the execution of the flattened term $t$ with the single node program state $\mathcal{S}$ will gives a state $\mathcal{S}'$ and value $v'$, where the flattened resulting distributed program state $|\mathcal{D}'|$ equals to the $S'$ and the flattened value $v$ equals to $v'$. Note that we make locations on all nodes distinct to simplify the proof.
% &\forall t \in \Pi_{\mathit{ref}}.\; \phi_\mathcal{D}, [t] \longrightarrow \mathcal{D}, [v] \iff \phi_\mathcal{S},  t|_@ \longrightarrow \mathcal{S}, v' \land \mathcal{S} = |\mathcal{D}| \land v' = v\\
\begin{gather*}
    \forall t \in \mathcal{T}.\; \mathcal{D}, [(n, t)] \longrightarrow \mathcal{D'}, [(n, v)] \land |\mathcal{D}| = \mathcal{S} \\ \Rightarrow \\ \mathcal{S},  t|_@ \longrightarrow \mathcal{S'}, v' \land |\mathcal{D}'| = \mathcal{S}' \land v|_@ = v'
\end{gather*}
\begin{align*}
    &\mathrm{where:}\\
    &|\mathcal{D}| = \bigcup_{\forall n \in \textbf{dom} \mathcal{D}} \mathcal{D}(n)\\
    &(\texttt{let mut}@n\;x=t; t)|_@ =  \texttt{let mut}\;x=t; t
    &\texttt{box}@n\;t |_@ = \texttt{box}\;t\\
    &\mathscr{l}^\bullet_{@n} \metaDef v |_@ = \mathscr{l}^\bullet \metaDef v
    &\mathscr{l}^\circ{@n} \metaDef v |_@ = \mathscr{l}^\circ \metaDef v\\
    &v@n|_@ = v
    &t@n|_@ = t |_@
\end{align*}
}%
\label{chap3:thm:loc-transp}
\end{theorem}
Since a term $t$ is always a closed term, we prove the theorem~\ref{chap3:thm:loc-transp} by structural induction on the term $t$. We focus on presenting proofs of the cases concerning the remote extensions in dFR's reduction rules given in figure~\ref{semantics:eval-distributed-1} and figure~\ref{semantics:eval-distributed-2}.
\begin{proof}[Case \textsc{\emph{Copy}}]
We assume that before the evaluation, given the distributed program state and monolithic program state as below:
\[\mathcal{D} = \mathcal{D}_0 \otimes (n' \mapsto (\mathscr{l}\mapsto (v, m)))\quad\quad
\mathcal{S} = \mathcal{S}_0 \otimes (\mathscr{l} \mapsto (v, m)) \quad\quad |\mathcal{D}_0| = \mathcal{S}_0\]
We can say that, initially, the flattened distributed program state equals to the monolithic program state: 
\[|\mathcal{D}| = \mathcal{S}\]
Following the dFR's reduction rule \textsc{Copy (s1)} and \textsc{Copy (s2)}, the distributed execution of a remote copy term gives:
\[\mathcal{D}_0 \otimes (n' \mapsto (\mathscr{l} \mapsto (v,m))) , [(n, !\mathscr{l}^\bullet_{@n'})] \longrightarrow \mathcal{D}_0 \otimes (n' \mapsto (\mathscr{l} \mapsto (v, m))), [(n, v@n')]\]
Following FR's reduction rule \textsc{Copy}, the monolithic execution of the flattened remote copy term gives:
\[\mathcal{S}_0 \otimes (\mathscr{l} \mapsto (v, m)), !\mathscr{l}^\bullet_{@n'}|_@ \longrightarrow \mathcal{S}_0 \otimes (\mathscr{l} \mapsto (v, m)), v\]
After the evaluation, the updated distributed program state and monolithic program state are shown as below:
\[\mathcal{D}' = \mathcal{D}_0 \otimes (n' \mapsto (\mathscr{l} \mapsto (v, m))) \quad\quad 
\mathcal{S}' = \mathcal{S}_0 \otimes (\mathscr{l} \mapsto (v, m))\]
To conclude, after the evaluation, the flattened updated distributed program state and the updated monolithic program state are equal, and the flattened resulting value obtained from the distributed execution and the value obtained from the monolithic execution are equal:
\[
|\mathcal{D}'| = \mathcal{S}' \quad\quad v@n'|_@ = v
\]
Hence:
\begin{gather*}
\mathcal{D}, [(n, !\mathscr{l}^\bullet_{@n'})] \longrightarrow \mathcal{D'}, [(n, v@n')] \land |\mathcal{D}| = \mathcal{S} \\\Rightarrow\\ \mathcal{S},  !\mathscr{l}^\bullet_{@n'}|_@ \longrightarrow \mathcal{S'}, v \land |\mathcal{D}'| = \mathcal{S}' \land v@n'|_@ = v
\end{gather*}


% \begin{align*}
%     &\text{We assume:}\\
%     &\mathcal{D} = \mathcal{D}_0 \otimes (n' \mapsto (l\mapsto (v, m)))\quad
%     \mathcal{S} = \mathcal{S}_0 \otimes (\mathscr{l} \mapsto (v, m))\\
%     &|\mathcal{D}| = \mathcal{S}\\
%     &\text{Following the dFR's reduction rule \textsc{Copy (s1)} and \textsc{Copy (s2)}, the distributed execution gives:}\\
%     &\mathcal{D}_0 \otimes (n' \mapsto (\mathscr{l} \mapsto (v,m))) , [(n, !\mathscr{l}^\bullet_{@n'})] \longrightarrow \mathcal{D}_0 \otimes (n' \mapsto (\mathscr{l} \mapsto (v, m))), [(n, v@n')]\\
%     &\text{Following the corresponding reduction rule, the monolithic execution gives:}\\
%     &\mathcal{S}_0 \otimes (\mathscr{l} \mapsto (v, m)), !\mathscr{l}^\bullet_{@n'}|_@ \longrightarrow \mathcal{S}_0 \otimes (\mathscr{l} \mapsto (v, m)), v\\
%     &\text{To conclude, after executions:}\\
%     &\mathcal{D}' = \mathcal{D}_0 \otimes (n' \mapsto (\mathscr{l} \mapsto (v, m))) \quad 
%     \mathcal{S}' = \mathcal{S}_0 \otimes (\mathscr{l} \mapsto (v, m))\\
%     &|\mathcal{D}'| = \mathcal{S}' \quad \text{and} \quad v@n'|_@ = v
% \end{align*}
\end{proof}
\begin{proof}[Case \textsc{\emph{Move}}]
We assume that before the evaluation, given the distributed program state and monolithic program state as below:
\[\mathcal{D} = \mathcal{D}_0 \otimes (n' \mapsto (\mathscr{l}\mapsto (v, m)))\quad\quad
\mathcal{S} = \mathcal{S}_0 \otimes (\mathscr{l} \mapsto (v, m))\quad\quad
|\mathcal{D}_0| = \mathcal{S}_0\]
We can say that, initially, the flattened distributed program state equals to the monolithic program state:
\[|\mathcal{D}| = \mathcal{S}\]
Following the dFR's reduction rule \textsc{Move (s1)} and \textsc{Move (s2)}, the distributed execution of a remote move term gives:
\[\mathcal{D}_0 \otimes (n' \mapsto (\mathscr{l} \mapsto (v,m))) , [(n, \#\mathscr{l}^\bullet_{@n'})] \longrightarrow \mathcal{D}_0 \otimes (n' \mapsto (\mathscr{l} \mapsto \bot), [(n, v@n')]\]
Following FR's reduction rule \textsc{Move}, the monolithic execution of the flattened remote move term gives:
\[\mathcal{S}_0 \otimes (\mathscr{l} \mapsto (v, m)), \#\mathscr{l}^\bullet_{@n'}|_@ \longrightarrow \mathcal{S}_0 \otimes (\mathscr{l} \mapsto \bot), v\]
After the evaluation, the updated distributed program state and monolithic program state are shown as below:
\[\mathcal{D}' = \mathcal{D}_0 \otimes (n' \mapsto (\mathscr{l} \mapsto \bot)) \quad\quad \mathcal{S}' = \mathcal{S}_0 \otimes (\mathscr{l} \mapsto \bot)\]
To conclude, after the evaluation, the flattened updated distributed program state and the updated monolithic program are equal, and the flatted resulting value obtained from the distributed execution and the resulting value obtained from the monolithic execution are equal:
\[|\mathcal{D}'| = |\mathcal{D}_0| \otimes (\mathscr{l} \mapsto \bot) = \mathcal{S}_0 \otimes (\mathscr{l} \mapsto \bot) = \mathcal{S}' \quad\quad v@n'|_@ = v\]
Hence:
\begin{gather*}
\mathcal{D}, [(n, \#\mathscr{l}^\bullet_{@n'})] \longrightarrow \mathcal{D'}, [(n, v@n')] \land |\mathcal{D}| = \mathcal{S} \\\Rightarrow\\ \mathcal{S},  \#\mathscr{l}^\bullet_{@n'}|_@ \longrightarrow \mathcal{S'}, v \land |\mathcal{D}'| = \mathcal{S}' \land v@n'|_@ = v
\end{gather*}
    % \begin{align*}
    % &\text{We assume:}\\
    % &\mathcal{D} = \mathcal{D}_0 \otimes (n' \mapsto (l\mapsto (v, m)))\quad
    % \mathcal{S} = \mathcal{S}_0 \otimes (\mathscr{l} \mapsto (v, m))\\
    % &|\mathcal{D}| = \mathcal{S}\\
    % &\text{The distributed execution gives:}\\
    % &\mathcal{D}_0 \otimes (n' \mapsto (\mathscr{l} \mapsto (v,m))) , [(n, \#\mathscr{l}^\bullet_{@n'})] \longrightarrow \mathcal{D}_0 \otimes (n' \mapsto (\mathscr{l} \mapsto \bot), [(n, v@n')]\\
    % &\text{The monolithic execution gives:}\\
    % &\mathcal{S}_0 \otimes (\mathscr{l} \mapsto (v, m)), \#\mathscr{l}^\bullet_{@n'}|_@ \longrightarrow \mathcal{S}_0 \otimes (\mathscr{l} \mapsto \bot), v\\
    % &\text{To conclude, after executions:}\\
    % &\mathcal{D}' = \mathcal{D}_0 \otimes (n' \mapsto (\mathscr{l} \mapsto \bot)) \quad 
    % \mathcal{S}' = \mathcal{S}_0 \otimes (\mathscr{l} \mapsto \bot)\\
    % &|\mathcal{D}'| = |\mathcal{D}_0| \otimes (\mathscr{l} \mapsto \bot) = \mathcal{S}_0 \otimes (\mathscr{l} \mapsto \bot) = \mathcal{S}' \quad \text{and} \quad v@n'|_@ = v
    % \end{align*}
\end{proof}
\begin{proof}[Case \textsc{\emph{Box}}]
We assume that before the evaluation, given the distributed program state $\mathcal{D}$ and monolithic program state $\mathcal{S}$ where the flatten distributed program state equals to the monolithic program state. In addition, we take a fresh location $\mathscr{l}$:
\[
|\mathcal{D}| = \mathcal{S} \quad\quad \mathscr{l} \notin \textbf{dom}\; \mathcal{D}(n') \quad\quad \mathscr{l} \notin \textbf{dom}\; \mathcal{S}
\]
Following the dFR's reduction rule \textsc{Box (s1)} and \textsc{Box (s2)}, the distributed execution of a remote heap allocation term gives:
\[
\mathcal{D}, [(n, \texttt{box}@n'\;v)] \longrightarrow \mathcal{D} \mid (n' \mapsto \mathcal{D}(n')\otimes(\mathscr{l} \mapsto (v, \top))), [(n, \mathscr{l}^\bullet_{@n'})]
\]
Following FR's reduction rule \textsc{Box}, the monolithic execution of the flattened remote heap allocation term gives:
\[
\mathcal{S}, (\texttt{box}@n'\;v)|_@ \longrightarrow \mathcal{S} \otimes (\mathscr{l} \mapsto (v, \top)), \mathscr{l}^\bullet
\]
After the evaluation, the updated distributed program state and monolithic program state are shown as below:
\[
\mathcal{D}' = \mathcal{D} \mid (n' \mapsto \mathcal{D}(n')\otimes(\mathscr{l} \mapsto (v, \top))) \quad\quad 
\mathcal{S}' = \mathcal{S} \otimes (\mathscr{l} \mapsto (v, \top))
\]
The updated distributed program state can be flatten into:
\[|\mathcal{D}'| = |\mathcal{D}| \otimes (\mathscr{l} \mapsto (v, \top))\]
Since we have $|\mathcal{D}| = \mathcal{S}$, we can conclude that the flattened updated distributed program state and the updated monolithic program state are equal:
\[|\mathcal{D}'| = |\mathcal{D}| \otimes (\mathscr{l} \mapsto (v, \top)) = \mathcal{S} \otimes (\mathscr{l} \mapsto (v, \top))= \mathcal{S}'\]
Also, the flattened remote owned reference obtained from the distributed execution equals to the owned reference obtained from the monolithic execution:
\[\mathscr{l}^\bullet_{@n'}|_@ = \mathscr{l}^\bullet\]
Hence:
\begin{gather*}
\mathcal{D}, [(n, \texttt{box}@n'\;v)] \longrightarrow \mathcal{D'}, [(n, \mathscr{l}^\bullet)] \land |\mathcal{D}| = \mathcal{S} \\\Rightarrow\\ \mathcal{S},  (\texttt{box}@n'\;v)|_@ \longrightarrow \mathcal{S'}, \mathscr{l}^\bullet \land |\mathcal{D}'| = \mathcal{S}' \land \mathscr{l}^\bullet_{@n'}|_@ = \mathscr{l}^\bullet
\end{gather*}
    % \begin{align*}
    % &\text{We assume:}\\
    % &|\mathcal{D}| = \mathcal{S} \quad \mathscr{l} \notin \textbf{dom}\; \mathcal{D}(n') \quad \mathscr{l} \notin \textbf{dom}\; \mathcal{S}\\
    % &\text{The execution of the distributed program gives:}\\
    % &\mathcal{D}, [(n, \texttt{box}@n'\;v)] \longrightarrow \mathcal{D} \mid (n' \mapsto \mathcal{D}(n')\otimes(\mathscr{l} \mapsto (v, \top))), [(n, \mathscr{l}^\bullet_{@n'})]\\
    % &\text{The execution of the monolithic program gives:}\\
    % &\mathcal{S}, (\texttt{box}@n'\;v)|_@ \longrightarrow \mathcal{S} \otimes (\mathscr{l} \mapsto (v, \top)), \mathscr{l}^\bullet\\
    % &\text{To conclude, after executions:}\\
    % &\mathcal{D}' = \mathcal{D}(n')\otimes(\mathscr{l} \mapsto (v, \top)) \quad 
    % \mathcal{S}' =  \mathcal{S} \otimes (\mathscr{l} \mapsto (v, \top))\\
    % &|\mathcal{D}'| = |\mathcal{D}| \otimes (\mathscr{l} \mapsto (v, \top)) = \mathcal{S} \otimes (\mathscr{l} \mapsto (v, \top))= \mathcal{S}' \quad \text{and} \quad \mathscr{l}^\bullet_{@n'}|_@ = \mathscr{l}^\bullet
    % \end{align*}
\end{proof}
\begin{proof}[Case \textsc{\emph{Borrow}}]
We assume that before the evaluation, given the distributed program state $\mathcal{D}$ and monolithic program state $\mathcal{S}$ where the flatten distributed program state equals to the monolithic program state. In addition, $\mathscr{l}$ is a location with an allocated value:
\[
|\mathcal{D}| = \mathcal{S} \quad\quad \mathscr{l} \in \textbf{dom}\; \mathcal{D}(n') \quad\quad \mathscr{l} \in \textbf{dom}\; \mathcal{S}
\]
Following the dFR's reduction rule \textsc{Borrow (s1)} and \textsc{Borrow (s2)}, the distributed execution of a remote borrow term gives:
\[
\mathcal{D}, [(n, \&[\texttt{mut}]\mathscr{l}^\bullet_{@n'})] \longrightarrow \mathcal{D}, [(n, \mathscr{l}^\circ_{@n'})]
\]
Following FR's reduction rule \textsc{Borrow}, the monolithic execution of the flattened remote borrow term gives:
\[
\mathcal{S}, (\&[\texttt{mut}]\mathscr{l}^\bullet_{@n'})|_@ \longrightarrow \mathcal{S}, \mathscr{l}^\circ
\]
To conclude, after the evaluation, the distributed program state and the monolithic program state remain unchanged, hence they are still equal. In addition, the flattened remote borrowed reference obtained from the distributed execution equals to the borrowed reference obtained from the monolithic execution:
\[
\mathscr{l}^\circ_{@n'}|_@ = \mathscr{l}^\circ
\]
Hence:
\begin{gather*}
\mathcal{D}, [(n, \&[\texttt{mut}]\mathscr{l}^\bullet_{@n'})] \longrightarrow \mathcal{D}, [(n, \mathscr{l}^\circ)] \land |\mathcal{D}| = \mathcal{S} \\\Rightarrow\\ \mathcal{S}, (\&[\texttt{mut}]\mathscr{l}^\bullet_{@n'})|_@ \longrightarrow \mathcal{S}, \mathscr{l}^\circ \land |\mathcal{D}| = \mathcal{S} \land \mathscr{l}^\circ_{@n'}|_@ = \mathscr{l}^\circ
\end{gather*}
    % \begin{align*}
    % &\text{We assume:}\\
    % &|\mathcal{D}| = \mathcal{S} \quad \mathscr{l} \in \textbf{dom}\; \mathcal{D}(n') \quad \mathscr{l} \in \textbf{dom}\; \mathcal{S}\\
    % &\text{The execution of the distributed program gives:}\\
    % &\mathcal{D}, [(n, \&[\texttt{mut}]\mathscr{l}^\bullet_{@n'})] \longrightarrow \mathcal{D} [(n, \mathscr{l}^\circ_{@n'})]\\
    % &\text{The execution of the monolithic program gives:}\\
    % &\mathcal{S}, (\&[\texttt{mut}]\mathscr{l}^\bullet_{@n'})|_@ \longrightarrow \mathcal{S}, \mathscr{l}^\circ\\
    % &\text{To conclude, after executions:}\\
    % &\mathcal{D}' = \mathcal{D}\quad 
    % \mathcal{S}' =  \mathcal{S}\quad
    % |\mathcal{D}'| = \mathcal{S}' \quad \text{and} \quad \mathscr{l}^\circ_{@n'}|_@ = \mathscr{l}^\circ
    % \end{align*}
\end{proof}
\begin{proof}[Case \textsc{\emph{Assign Owned}}]
We assume that before the evaluation, given the distributed program state and monolithic program state as below:
\[\mathcal{D} = \mathcal{D}_0 \otimes (n' \mapsto (\mathscr{l}\mapsto (v, m)))\quad\quad
\mathcal{S} = \mathcal{S}_0 \otimes (\mathscr{l} \mapsto (v, m))\quad\quad
|\mathcal{D}_0| = \mathcal{S}_0\]
We can say that, initially, the flattened distributed program state equals to the monolithic program state:
\[|\mathcal{D}| = \mathcal{S}\]
Following the dFR's reduction rule \textsc{Assign Owned (s1)} and \textsc{Assign Owned (s2)}, the distributed execution of a remote assignment to an owned reference gives:
\[
\mathcal{D}_0 \otimes (n' \mapsto (\mathscr{l} \mapsto (v,m))) , [(n, \mathscr{l}^\bullet_{@n'} \metaDef v')] \longrightarrow \mathcal{D}_0 \otimes (n' \mapsto (\mathscr{l} \mapsto (v',m))), [(n, \texttt{()}@n')]
\]
Following FR's reduction rule \textsc{Assign Owned}, the monolithic execution of the flattened remote assignment to an owned reference gives:
\[
\mathcal{S}_0 \otimes (\mathscr{l} \mapsto (v, m)), (\mathscr{l}^\bullet_{@n'} \metaDef v')|_@ \longrightarrow \mathcal{S}_0 \otimes (\mathscr{l} \mapsto (v', m)), \texttt{()}
\]
After the evaluation, the updated distributed program state and monolithic program state are shown as below:
\[
\mathcal{D}' = \mathcal{D}_0 \otimes (n' \mapsto (\mathscr{l} \mapsto (v',m))) \quad\quad 
\mathcal{S}' = \mathcal{S}_0 \otimes (\mathscr{l} \mapsto (v', m))
\]
To conclude, after the evaluation, the flattened updated distributed program state and the updated monolithic program are equal, and the flatted unit value obtained from the distributed execution and the unit value obtained from the monolithic execution are trivially equal:
\[
|\mathcal{D}'| = |\mathcal{D}_0| \otimes (\mathscr{l} \mapsto (v', m)) = \mathcal{S}_0 \otimes (\mathscr{l} \mapsto (v', m)) = \mathcal{S}' \quad\quad \texttt{()}@n'|_@ = \texttt{()}
\]
Hence:
\begin{gather*}
\mathcal{D}, [(n, \mathscr{l}^\bullet_{@n'} \metaDef v')] \longrightarrow \mathcal{D'}, [(n, \texttt{()}@n')] \land |\mathcal{D}| = \mathcal{S} \\\Rightarrow\\ \mathcal{S},  (\mathscr{l}^\bullet_{@n'} \metaDef v')|_@ \longrightarrow \mathcal{S'}, \texttt{()} \land |\mathcal{D}'| = \mathcal{S}' \land \texttt{()}@n'|_@ = \texttt{()}
\end{gather*}
    % \begin{align*}
    % &\text{We assume:}\\
    % &\mathcal{D} = \mathcal{D}_0 \otimes (n' \mapsto (l\mapsto (v, m)))\quad
    % \mathcal{S} = \mathcal{S}_0 \otimes (\mathscr{l} \mapsto (v, m))\\
    % &|\mathcal{D}| = \mathcal{S}\\
    % &\text{The execution of the distributed program gives:}\\
    % &\mathcal{D}_0 \otimes (n' \mapsto (\mathscr{l} \mapsto (v,m))) , [(n, \mathscr{l}^\bullet_{@n'} \metaDef v')] \\&\longrightarrow \mathcal{D}_0 \otimes (n' \mapsto (\mathscr{l} \mapsto (v',m))), [(n, \texttt{()}@n')]\\
    % &\text{The execution of the monolithic program gives:}\\
    % &\mathcal{S}_0 \otimes (\mathscr{l} \mapsto (v, m)), (\mathscr{l}^\bullet_{@n'} \metaDef v')|_@ \longrightarrow \mathcal{S}_0 \otimes (\mathscr{l} \mapsto (v', m)), \texttt{()}\\
    % &\text{To conclude, after executions:}\\
    % &\mathcal{D}' = \mathcal{D}_0 \otimes (n' \mapsto (\mathscr{l} \mapsto (v',m))) \quad 
    % \mathcal{S}' = \mathcal{S}_0 \otimes (\mathscr{l} \mapsto (v', m))\\
    % &|\mathcal{D}'| = |\mathcal{D}_0| \otimes (\mathscr{l} \mapsto (v', m)) = \mathcal{S}_0 \otimes (\mathscr{l} \mapsto (v', m)) = \mathcal{S}' \quad \text{and} \quad \texttt{()}@n'|_@ = \texttt{()}
    % \end{align*}
\end{proof}
\begin{proof}[Case \textsc{\emph{Assign Borrowed}}]
We assume that before the evaluation, given the distributed program state and monolithic program state as below:
\[\mathcal{D} = \mathcal{D}_0 \otimes (n' \mapsto (\mathscr{l}\mapsto (v, m)))\quad\quad
\mathcal{S} = \mathcal{S}_0 \otimes (\mathscr{l} \mapsto (v, m))\quad\quad
|\mathcal{D}_0| = \mathcal{S}_0\]
We can say that, initially, the flattened distributed program state equals to the monolithic program state:
\[|\mathcal{D}| = \mathcal{S}\]
Following the dFR's reduction rule \textsc{Assign Borrowed (s1)} and \textsc{Assign Borrowed (s2)}, the distributed execution of a remote assignment to a (mutably) borrowed reference gives:
\[
\mathcal{D}_0 \otimes (n' \mapsto (\mathscr{l} \mapsto (v,m))) , [(n, \mathscr{l}^\circ_{@n'} \metaDef v')] \longrightarrow \mathcal{D}_0 \otimes (n' \mapsto (\mathscr{l} \mapsto (v',m))), [(n, \texttt{()}@n')]
\]
Following FR's reduction rule \textsc{Assign Borrowed}, the monolithic execution of the flattened remote assignment to a (mutably) borrowed reference gives:
\[
\mathcal{S}_0 \otimes (\mathscr{l} \mapsto (v, m)), (\mathscr{l}^\circ_{@n'} \metaDef v')|_@ \longrightarrow \mathcal{S}_0 \otimes (\mathscr{l} \mapsto (v', m)), \texttt{()}
\]
After the evaluation, the updated distributed program state and monolithic program state are shown as below:
\[
\mathcal{D}' = \mathcal{D}_0 \otimes (n' \mapsto (\mathscr{l} \mapsto (v',m))) \quad\quad 
\mathcal{S}' = \mathcal{S}_0 \otimes (\mathscr{l} \mapsto (v', m))
\]
To conclude, after the evaluation, the flattened updated distributed program state and the updated monolithic program are equal, and the flatted unit value obtained from the distributed execution and the unit value obtained from the monolithic execution are trivially equal:
\[
|\mathcal{D}'| = |\mathcal{D}_0| \otimes (\mathscr{l} \mapsto (v', m)) = \mathcal{S}_0 \otimes (\mathscr{l} \mapsto (v', m)) = \mathcal{S}' \quad\quad \texttt{()}@n'|_@ = \texttt{()}
\]
Hence:
\begin{gather*}
\mathcal{D}, [(n, \mathscr{l}^\circ_{@n'} \metaDef v')] \longrightarrow \mathcal{D'}, [(n, \texttt{()}@n')] \land |\mathcal{D}| = \mathcal{S} \\\Rightarrow\\ \mathcal{S},  (\mathscr{l}^\circ_{@n'} \metaDef v')|_@ \longrightarrow \mathcal{S'}, \texttt{()} \land |\mathcal{D}'| = \mathcal{S}' \land \texttt{()}@n'|_@ = \texttt{()}
\end{gather*}
\end{proof}
\begin{proof}[Case \textsc{\emph{Decl}}]
We assume that before the evaluation, given the distributed program state $\mathcal{D}$ and monolithic program state $\mathcal{S}$ where the flatten distributed program state equals to the monolithic program state. In addition, we take a fresh location $\mathscr{l}$:
\[
|\mathcal{D}| = \mathcal{S} \quad\quad \mathscr{l} \notin \textbf{dom}\; \mathcal{D}(n') \quad\quad \mathscr{l} \notin \textbf{dom}\; \mathcal{S}
\]
Following the dFR's reduction rule \textsc{Decl (s1)} and \textsc{Decl (s2)}, the distributed execution of a remote declaration gives:
\[
\mathcal{D}, [(n, \texttt{let}\;\texttt{mut}@n'\;x = v;t)] \longrightarrow \mathcal{D} \mid (n' \mapsto \mathcal{D}(n')\otimes(\mathscr{l} \mapsto (v, k))), [(n, t[x/\mathscr{l}^\bullet_{@n'}])]
\]
Following FR's reduction rule \textsc{Decl}, the monolithic execution of the flattened remote declaration gives:
\[
\mathcal{S}, (\texttt{let}\;\texttt{mut}@n'\;x = v;t)|_@ \longrightarrow \mathcal{S} \otimes (\mathscr{l} \mapsto (v, k)), t[x/\mathscr{l}^\bullet]
\]
After the evaluation, the updated distributed program state and monolithic program state are shown as below:
\[
\mathcal{D}' = \mathcal{D} \mid (n' \mapsto \mathcal{D}(n')\otimes(\mathscr{l} \mapsto (v, k))) \quad\quad
\mathcal{S}' = \mathcal{S} \otimes (\mathscr{l} \mapsto (v, k))
\]
To conclude, after the evaluation, the flattened updated distributed program state and the updated monolithic program are equal, and the substitution with the flattened owned reference obtained from the distributed execution and the substitution with the owned reference obtained from the monolithic execution are trivially equal:
\[
|\mathcal{D}'| = |\mathcal{D}| \otimes (\mathscr{l} \mapsto (v, k)) = \mathcal{S} \otimes (\mathscr{l} \mapsto (v, k)) = \mathcal{S}'\]\[ t[x/\mathscr{l}^\bullet_{@n'}]|_@ = t[x/(\mathscr{l}^\bullet_{@n'})|_@] = t[x/\mathscr{l}^\bullet]
\]
Hence:
\begin{gather*}
\mathcal{D}, [(n, \texttt{let}\;\texttt{mut}@n'\;x = v;t)] \longrightarrow \mathcal{D'}, [(n, t[x/\mathscr{l}^\bullet_{@n'}])] \land |\mathcal{D}| = \mathcal{S} \\\Rightarrow\\ \mathcal{S}, (\texttt{let}\;\texttt{mut}@n'\;x = v;t)|_@ \longrightarrow \mathcal{S'}, t[x/\mathscr{l}^\bullet] \land |\mathcal{D}'| = \mathcal{S}' \land t[x/\mathscr{l}^\bullet_{@n'}]|_@ = t[x/\mathscr{l}^\bullet]
\end{gather*}
    % \begin{align*}
    % &\text{We assume:}\\
    % &|\mathcal{D}| = \mathcal{S} \quad \mathscr{l} \notin \textbf{dom}\; \mathcal{D}(n') \quad \mathscr{l} \notin \textbf{dom}\; \mathcal{S}\\
    % &\text{The execution of the distributed program gives:}\\
    % &\mathcal{D}, [(n, \texttt{let mut}@n' x = v;t)] \\&\longrightarrow \mathcal{D} \mid (n' \mapsto \mathcal{D}(n')\otimes(\mathscr{l} \mapsto (v, k))), [(n, t[x/\mathscr{l}^\bullet_{@n'}])]\\
    % &\text{The execution of the monolithic program gives:}\\
    % &\mathcal{S}, (\texttt{let mut}@n' x = v;t)|_@ \longrightarrow \mathcal{S} \otimes (\mathscr{l} \mapsto (v, k)), t[x/\mathscr{l}^\bullet]\\
    % &\text{To conclude, after executions:}\\
    % &\mathcal{D}' = \mathcal{D} \mid (n' \mapsto \mathcal{D}(n')\otimes(\mathscr{l} \mapsto (v, k))) \quad 
    % \mathcal{S}' = \mathcal{S} \otimes (\mathscr{l} \mapsto (v, k))\\
    % &|\mathcal{D}'| = |\mathcal{D}| \otimes (\mathscr{l} \mapsto (v, k)) = \mathcal{S} \otimes (\mathscr{l} \mapsto (v, k)) = \mathcal{S}' \quad \\&\text{and} \quad t[x/\mathscr{l}^\bullet_{@n'}]|_@ = t[x/(\mathscr{l}^\bullet_{@n'})|_@] = t[x/\mathscr{l}^\bullet]
    % \end{align*}
\end{proof}
The proofs for location transparency of the distributed and monolithic executions of local terms and remote terms should then be trivial.

\subsection{Summary}
\label{chap3:semantics:summary}
In section~\ref{chap3:implementation}, we have discussed the design and implementation of a UMI framework as a library in Rust. The core designed concepts of such a library --- extending Rust's memory safety guarantees into a distributed setting --- is presented in the formalisation of a distributed extension of a core calculus of Rust in this section. By proving the location transparency theorem~\ref{chap3:thm:loc-transp}, we demonstrate that a distributed program developed using the UMI framework preserves the semantics of a monolithic program from which it is translated. Therefore, with our UMI framework, Rust's memory safety guarantees provided by its type system, lifetime and ownership system, and borrow checking mechanism are indeed extended into a distributed setting.

\section{Related Work}
\label{chap3:related-work}
\paragraph*{Design and Implementations of Remote Procedural Call}
Described in \citepos{10.5555/910306} thesis, RPC allows programs in separate address spaces to communicate synchronously. By experimenting with different implementations of RPC, \citet{10.5555/910306} argues that RPC is an efficient and effective programming tool for distributed systems. Java RMI~\citep{10.5555/1268049.1268066} implements the concept of RPC in a object-orientated programming language. It outlines a model for distributed objects within the Java environment which allows Java objects to communicate across different address spaces. This RMI framework is designed to integrate seamless with the Java language, preserving as much of the Java object model's semantics as possible. As for its memory management mechanism, it contains a design of distributed garbage collection, ensuring that remote objects which are no longer referenced by any client should be automatically garbage collected. However, the Stub object in this framework does not always preserves the semantics of the Java object model. In our UMI approach, the framework is designed to be truly integrated with Rust as it is semantic preserving. The idea of location transparency and seamless integration of distributed computing presented in this UMI framework is related on the formal model Krivine Nets presented by~\citet{10.1145/2628136.2628152}, which extends the classic Krivine abstract machine to support distributed execution. This extension allows a seamless and transparent RPC mechanism to handle higher-order functions without transmitting the actual code. Krivine Nets enable the seamless integration of distributed computing into programming languages by eliminating explicit communication and process management from source code. In addition, instead of embedding deployment details within the source code, Krivine Nets handle these details automatically.

\paragraph*{Distributed Memory Management}
Distributed garbage collection used in Java RMI~\citep{10.5555/1268049.1268066} as been an essential yet challenging research topic in distributed memory management. \citet{10.1145/292469.292471} offers a comprehensive review of distributed garbage collection (GC) schemes applicable to autonomous systems connected by a network, particularly in the context of Internet programming languages such as Java. It highlights the evolution of garbage collection from single-address-space collectors to distributed systems due to the increasing prominence of languages like Java in Internet applications. It categorises and reviews various GC methods, discussing their adaptation for distributed environments, which are characterised by issues such as communication overhead, locality of action, and non-deterministic communication latency. Designed for the object-oriented programming language with actor model Pony~\citep{10.1145/3133896}, Orca is a concurrent garbage collection algorithm which manages memory without requiring stop-the-world pauses or synchronisation mechanisms, enabling zero-copy message passing and mutable data sharing among actors. It leverages Pony's type system to ensure data race-free programs, allowing garbage collection tasks to be handled locally by each actor without synchronisation, thus enhancing performance and responsiveness.
Perhaps more relevant to our memory management mechanism, the region system Reggio~\citep{10.1145/3622846} accompanied with a type system for Verona, which is a concurrent object-oriented programming language, organises objects into isolated regions, each with its own memory management strategy. It addresses the challenge of providing the control of manual memory management while maintaining memory safety by utilising a combination of region-based memory partitioning and an ownership type system.

\paragraph*{Formalisation of Rust}
The formalism of a core calculus of the UMI framework presented in this chapter is based on a formalisation of a core calculus of Rust described by~\citet{10.1145/3443420}. Although we are not completely satisfied with this formalism, it is the most suitable choice comparing to all common approaches of formalisation of Rust's semantics we have surveyed.

There are many different approaches to formalise Rust, from different perspectives and aiming for different application domains. RustBelt~\citep{10.1145/3158154} provide a formalised continuation-passing style MIR --- $\lambda_{\textit{Rust}}$ --- mechanised using Iris. However, such an approach does not provide a formal model which is close to the source-level language of Rust. Hence, it is not convenient for us to use it as the basis for formalising the distributed extensions of the features of the UMI framework.

Different from RustBelt, there are also attempts of formalising Rust from a source-level language perspective. For instance, Oxide~\citep{weiss2021oxide} attempts to formalise near-complete source-level Rust language features, providing a type system and small-step operational semantics. An implementation of Oxide is provided on GitHub although the formal claims are not mechanised. We argue that due to the level of the complexity and obscurity of this formalism, it is rather hard for us to gain a clear understanding of the modelling of Rust's borrow checking and non-lexical lifetime. In addition, without a concise and consistent presentation of the syntax, type system and formal semantics of the core features of Rust, it is again hard for us to use such a formalism to reason about properties of Rust programs and be convinced that the type system is indeed sound.

Back to the formalisation we choose to build our distributed language extension upon, instead of modelling the full Rust language, \citet{10.1145/3443420} formalises a core calculus of Rust, which is FR, emphasising on the understanding of borrowing and lifetime of Rust. However, this formalism does not include essential Rust language features such as tuples, structs, functions and closures which makes it too minimalist. One may argue that \citet{10.1145/3443420} does discuss the possible extensions of FR to include tuples and functions hence it is unfair to criticise such an approach being ``too minimalist", however, I insist that these features are fundamental building blocks of a core language and should not be treated as extensions of a core language. Besides, the way that the let-binding is modelled made it hard to do substitutions, which is in my opinion the obstacle of having functions as a part of the formalism. In addition, its lexical treatment of Rust's lifetime is already obsolete.

Due to the reality of lack of a concise formalism of the source-level Rust that captures all key concepts of Rust's language features, our formalism of the UMI framework as a distributed extension of Rust is also not completely satisfying. However, it does demonstrate Rust's core memory management mechanisms, and allows us to prove the semantic preservation property of distributed programs implemented using the UMI framework. To have formalism of UMI capturing more language features would require us to develop yet another formalism of source-level Rust, which is out of the scope of this project.

% \section{Limitation and Further Work}
% \label{chap3:limitation-further-work}
% There are some limitations of both the practical implementation and the formalisation of the semantics worth discussing. As for the design and implementation of the UMI library, mechanisms of handling internet communication errors are not presented in the design of the framework.

\section{Conclusion}
\label{chap3:conclusion}
In this chapter, we firstly present our design, implementation, and formalisation of a UMI framework for Rust. This UMI framework allows programmers to express distributed computation in the same form of monolithic computation, abstracting away the internet communication complications and message passing details. To demonstrate as a distributed extension of Rust, our UMI framework extends Rust's memory safety guarantees into a distributed setting, we present the formalism of a core calculus focusing on the core features relating to distributed memory management mechanisms in the UMI framework. We present FR, which is a core calculus of the surface language of Rust, formalising the key concepts of Rust's ownership and lifetime in memory management. Then we extend FR into dFR, to include distributed features of the UMI framework. By showing that distributed programs written in dFR preserves the semantics of monolithic programs written in FR via proving a location transparency theorem, we can conclude that the memory safety guarantees provided by Rust can be extended to distributed programs written with our UMI framework.

\noindent
\begin{center}
\vspace{0.3em}
\begin{tikzpicture}[color=black,
                   transform shape,
                   every node/.style={inner sep=0pt}]
\node[minimum width=0.35\framesize, minimum height=0.05*\framesize, fill=white](vecbox){};
\node[anchor=west] at (vecbox.west){\pgfornament[width=0.08*\framesize]{16}};
\node[anchor=east] at (vecbox.east){\pgfornament[width=0.08*\framesize]{15}};
\node[inner sep=6pt] (text) at (vecbox.center){\textbf{\Large Epilogue}};
\end{tikzpicture}
\vspace{-0.7em}
\end{center}
In this study, we have designed and implemented a distributed computing framework in Rust which allows distributed programs to preserve the semantics of monolithic programs. Such a design makes it easier for programmers to migrate monolithic Rust programs into distributed setting while adopting the memory safety guarantees of these monolithic programs. In the end, I would like to enclose this chapter with some high-level discussions.

Our UMI framework provides a new perspective for characterising the relationship between a monolithic program and a distributed program --- a monolithic program can be viewed as an abstraction of a distributed program. It specifies the intend functionalities to be achieved by a distributed program while encapsulating the detailed implementations message passing, data serialisation and deserialisation, and other network communication complications.

However, with such an abstract view of distributed program, network communication errors such as timeouts and server errors are not explicitly modelled or handled with the UMI framework. Although we envision integrating such a framework within micro-services platforms where server errors are mitigated by cloud service providers, and supervision strategies can by used for taking snapshots and restarting from a failure, making these issues not a critical problem of the design of the UMI framework, we have to admit that, such a framework itself is not expressive enough to model network communication errors.

One observation is that there is a trade-off between abstraction and expressiveness in modelling a distributed system. In this study, by making external facilities for handling the network communication issues, we focus on designing a concise abstraction which describes the intended functionalities of distributed programs and making it easier to model and reason about memory safety of distributed programs.