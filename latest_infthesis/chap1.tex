\chapter{Climbing Tower of Babel}
\label{chap1}
\begin{tikzpicture}[color=black,
                   transform shape,
                   every node/.style={inner sep=0pt}]
\node[minimum width=\framesize, minimum height=0.55 *\framesize, fill=white](vecbox){};
\node[anchor=north west] at (vecbox.north west){% 
\pgfornament[width=0.1*\framesize]{61}};
\node[anchor=north east] at (vecbox.north east){% 
\pgfornament[width=0.1*\framesize,symmetry=v]{61}};
\node[anchor=south west] at (vecbox.south west){% 
\pgfornament[width=0.1*\framesize,symmetry=h]{61}};
\node[anchor=south east] at (vecbox.south east){% 
\pgfornament[width=0.1*\framesize,symmetry=c]{61}};
\node[anchor=north] at (vecbox.north){% 
\pgfornament[width=0.25*\framesize]{88}
\pgfornament[width=0.25*\framesize]{88}
\pgfornament[width=0.25*\framesize]{88}
};

\node[anchor=south] at (vecbox.south){% 
\pgfornament[width=0.25*\framesize]{88}
\pgfornament[width=0.25*\framesize]{88}
\pgfornament[width=0.25*\framesize]{88}
};
\node[text width=0.85\framesize, align=justify] at (vecbox.center){%
The \Lord said, ``If as one people speaking the same language they have begun to do this, then nothing they plan to do will be impossible for them. Come, let us go down and confuse their language so they will not understand each other."
\\
So the \Lord scattered them from there over all the earth, and they stopped building the city. That is why it was called \emph{Babel} --- because there the \Lord \emph{confused} the language of the whole world. From there the \Lord scattered them over the face of the whole earth.\\
\rightline{--- Genesis 11:1â€“9 NRSVUE}};
\end{tikzpicture}
% Humans are divided into different linguistic groups, therefore, they are unable to understand each other. Yet, there are people trying to understand others, from a linguistic perspective, a cognitive perspective, a psychological perspective, a social perspective etc. Naturally, the gap between humans and machines is wider than the gap between different human linguistic groups. Again, there are people studying different layers of machine languages and the process of translating (i.e., compilation and interpretation) human languages into machine codes. 
% THE MYTHOLOGY %
\lettrine{H}{umans} are divided into different linguistic groups, therefore, they are unable to understand each other. That said, ``meaning" is encoded into different conceptual schemes in different languages; without accurate translations, it is ultimately difficult for people who speak different languages to communicate with each others. In addition, due to the nature of natural language being ambiguous, it is hard to accurately reason about what others \emph{really mean} even under the context of a same language.

% PHLOSOPHY %
Over the decades, there has been various studies concerning the \emph{meaning} of languages. The term \emph{semantics} is used to refer to the studies of linguistic meaning~\citep{katz1972, palmer1981semantics}. From a philosophical perspective, there are different theories of meaning. \citet{lewis1970} describes two topics of the studies of meaning. Corresponding to the first observation from the mythology --- ``meaning is encoded into different conceptual schemes in different languages" --- the first topic concerning the meaning of languages is to understand the psychological and sociological facts that a person or a group of people give certain meanings to the symbols in their languages~\citep{lewis1970}. One kind of approaches are \emph{ideational theories}~\citep{ChapmanRoutledge+2009+84+85}, which examines the meaning in terms of and as an output of people's mental representations~\citep{Stich1994-STIMR}. A different point of view is initiated by \citet{Kripke1980-KRINAN}, who argues against the idea of proper name being synonymous with definite descriptions, while proposing that names are associated with their referents through a causal chain of reference. Such a \emph{causal theory} further suggests that the meaning of an expression instead of being inherited from mental states, is determined by the causal connections that the expression has with the objects or concepts that it~refers~to. 

Corresponding to the second observation --- ``it is hard to accurately reason about what others really mean even under the context of a same language" --- the second topic is to accurately examine and analyse the meaning of an expression (i.e., a word or a sentence) in a given language~\citep{lewis1970}. Specifically, \citet{frege1892} introduces a \emph{theory of reference}, suggesting that meaning of a expression involves both its reference to an object, which is a proper name that contributes to the truth value of a sentence, and its sense, which is how the object is presented. Using the example sentence ``the present King of France is bald", \emph{Russell's theory of description}~\citep{russell1904} argues that Frege's notion of sense and reference is not sufficient for analysing an expression which has sense but no reference, while introducing a rigorous analytic method for problematic propositions, concerning denoting phrases, making use of the machinery of first-order logic featuring propositional functions. Following \citepos{tarski1944} truth definition of a sentence, \citet{davidson1967} proposes an approach with the core idea that meaning should be understood based on a \emph{formal theory of truth}. There are also \emph{semantic internalism} theories~\citep{Mcgilvray1998, Chomsky2000, pietroski2017semantic} that instead of giving truth value to expressions, view the meaning of an expression as what is used for building a particular mental representation. Taking a holistic approach to analyse the meaning of expressions, \emph{inferential semantics} theories~\citep{Brandom2000} argue against the idea using establish truth conditions to further analyse good and bad inferences, instead, suggest to first study the distinction between good and bad inferences, which provides the basis for understanding truth conditions. Hence the meaning of an expression is studied in relation to other~expressions. 
% There are topic knowledge representation, semantic parsing
\begin{center}
\vspace{-0.7em}
\pgfornament[width=0.08*\framesize]{11}
\pgfornament[width=0.08*\framesize]{80}
\pgfornament[width=0.08*\framesize]{14}
\vspace{-0.3em}
\end{center}

% LINGUISTICS %
Linguists tend to adopt less abstract approaches to analyse the meaning of languages. There are also many different topics within linguistic studies of semantics.

One important field of linguistic semantics is \emph{lexical semantics}, which concerns the meaning of words~\citep{palmer1981semantics, PUSTEJOVSKY200698, LexicalSemantics}, including topics such as the \emph{semantic structure} of words like ambiguity and polysemy as well as the semantic relations between words such as metaphor and metonymy, \emph{lexical fields} (alternatively \emph{semantic fields}) which has been initially introduced by \citet{trier1931deutsche} studying the meaning of words according to their relationship to other words of which the meanings are interdependent~\citep{palmer1981semantics, jackson2000words}, and \emph{lexical relations} which studies the structural relation between words like synonymy and antonymy~\citep{LexicalSemantics}.

Another widely explored field of linguistic semantics is \emph{structural semantics}, or more general, \emph{structural linguistics}, which is inspired by \citepos{Saussure1916} semiotic analysis centring linguistic signs, attempting to analyse a language as a structured system of interrelated elements. In particular, the aforementioned topics like semantic fields and lexical relations along with other semantic relation between words have been taken from the lexical studies and further developed into a structured basis for the analysis or words' meaning~\citep{LexicalSemantics}. Such an influential approach has later been adapted into the studies of generative grammars and their formalism~\citep{Katz1963-KATTSO-3, Chomsky1975-CHOTLS}.

\emph{Cognitive semantics} is a major part of cognitive linguistics, which is an important linguistic field has been explored through decades by \citet{Johnson1987-JOHTBI}, \citet{alma9923109163502466}, \citet{alma993245163502466}, \citet{fauconnier1998}. taking an alternative approach from the structural linguistics, it studies the meaning of languages under the context of human cognition, specifically, viewing semantics as a conceptual organisation of languages~\citep{10.7551/mitpress/6847.001.0001, Croft_Cruse_2004}.

Perhaps along with the emerging natural language processing technology such as ChatGPT, the field \emph{computational linguistics}, concerning modelling natural languages as computational models, has now become one of most popuar areas. \emph{Computational semantics}, as an important study within the scope of the computational linguistics, enable automatic analysis of sentences' meaning via machines~\citep{mitkov2022}. Base on the aforementioned fields such as lexical semantics and structural semantics as well as formal semantics like Montague semantics~\citep{Montague1970-MONEAA-2} which will be later discussed, the main focus of this field is the \emph{representation} of meaning, where the meaning of the languages are represented via some formal structures, for instance, logic forms including propositional logic~\citep{boole1854investigation} and first order predicate logic~\citep{Frege1879-FREBAF-2}, discourse representation structure studies in discourse representation theory~\citep{Kamp1993-KAMFDT}, and event structures~\citep{PUSTEJOVSKY199147}, tense logic~\citep{Prior1955-PRITAM, Kamp1968-KAMTLA} as well as the temporal anaphora that provides representations of events and time in sentences~\citep{partee1884, hinrichs1986}. Semantic parsing is the technique being studied to transform sentences into these semantic representations, while semantic analysis and semantic inference are perform on these semantic representations for automatically processing the meaning of sentences. 

Originated from philosophical view, especially the logic of languages, linguistic formal semantics focusing on analysing the truth condition aspect of meaning with frameworks concerning compositionality, specifically, a formal analysis of the semantics of some language is achieve base on a syntactic formalism of a language~\citep{alma999704883502466}. Although obviously the study of linguistic formal semantics does not intend to explore all features of the semantics of the natural languages, it explores ways for precise modelling of the syntax as well as analysis of semantics of sentences. An important work within such an field is categorial grammar, of which \citepos{Adj35} development provides a formal syntactic approach for analysing higher-order logic. Such an approach has been later adapted for analysing natural languages, such as Lambek calculus~\citep{Lambek1958-LAMTMO-5}. Montague semantics, which is a model-theoretical approach, providing a relation between syntax and semantics~\citep{Montague1970-MONEAA-2}. It analyses a subset of English, taking the form of Montague grammar, with lambda calculus, higher-order functions, and type theory. Combintorial categorial grammar~\citep{steedman2001, steedman2011combinatory} further provides formalism of categorial grammar with combintorial logic, which shares the same level of expressiveness as lambda calculus.
\begin{center}
\vspace{-0.7em}
\pgfornament[width=0.08*\framesize]{11}
\pgfornament[width=0.08*\framesize]{80}
\pgfornament[width=0.08*\framesize]{14}
\vspace{-0.3em}
\end{center}

% From Natural Languages to Programming Languages
As we have discussed in the previous paragraphs, within the scope of the linguistic formal studies concerning structured and systematic analysis of the meaning of languages including computational and formal semantics, various formal systems and logic frameworks such as lambda calculus, first order logic, modal logic, combintorial logic, category theory etc. are used in modelling formal syntactical representations of natural languages to enable precise analysis of the semanticsas well as to facilitate machines to understand and generate meaningful structured sentences. Perhaps not surprisingly, these formal systems and logic components also form an important foundation for the design and analysis of \emph{programming languages}.

% COMPUTER SCIENCE %
% Naturally, the gap between humans and computers is even larger than the gap between different human linguistic groups. 
Programming languages are created by human, serving as an interface for human to communicate and interact with computers. Since humans tend to encode and express information in terms of structured phrases and sentences, like natural languages, programming languages are designed to have grammar and syntax for human to convey their intentions to computers taking the form \emph{computer programs}. While computers execute binary code containing zeros and ones, the communication processes between humans and computers are facilitated by compilers, which are responsible for translating information encoded by humans taking the form of computer programs into executable machine code. The study of programming languages explores different approaches for effectively expressing better abstractions in various application domains, facilitating accurate and efficient compilation process, and providing better frameworks for humans to understand and reason about the behaviours of computers' executions of programs.

% Before stepping into the clich\'e of introducing the formal semantics of programming languages, a question is necessary to be asked. We keep saying that the study of semantics is the study of the meaning of languages, however, \emph{what is ``meaning"?} Within the scope of programming language studies, it is: 0) how we \emph{model} and \emph{understand} programs; 1) how we \emph{communicate} what we want computers to do in terms of programs with computers; 2) how we \emph{reason about} the behaviours of programs.

The study of formal semantics of programming languages has been around for decades and three main forms of formal semantics have been used serving as a foundation for understanding and reasoning about computer programs~\citep{10.5555/151145, citeulike:105547}. Specifically, \emph{operational semantics} provides meanings to programs by modelling how computations get executed. In particular, \emph{small-step operational semantics} focuses on the incremental reduction of expressions or states, providing a detailed and precise understanding of program behaviours, while \emph{big-step operational semantics} describes the execution of programs in terms of its overall behaviours or outcomes rather than its individual intermediate execution steps. Operational semantics is particularly useful for the implementation of a programming language. \emph{Denotational semantics} gives meanings to programs by modelling the result of computations as mathematical objects. It abstract away the details of the execution of programs and gives an elegant mathematical model presenting the core concepts of a programming language. Instead of modelling how computations get executed or what are produced by executions of computations, \emph{axiomatic semantics} provides meanings to programs by specifying properties satisfied by the results produced by executions of computations. In practice, it is particularly useful for building a proof system for reasoning about the execution of programs.

In my three-year short research journey, my fundamental motivations are to gain precise understanding of humans' mental models of computer programs and to improve the design of programming languages in order to allow humans to effectively communicate with computers. Corresponding to the foundational programming languages semantics study, I intend to explore ways to design better abstractions to \emph{model} and \emph{understand} programs in order to allow programmers to effectively \emph{communicate} what they want computers to do in terms of programs and precise formal frameworks allowing us to \emph{reason about} the behaviours of complicated realistic programs.
Till the end of this journey, I have asked three conceptual questions, and conducted three studies concerning the meaning of computer programs, utilising modelling and reasoning techniques presented in the existing studies of formal semantics of programming languages.

% Hence, it is important to study the \emph{meaning} of programs expressed using these languages, i.e., the semantics of programming languages.
% As a computer scientist who is specialised in the study of programming languages, through this three-year short research journey, my fundamental motivations are to gain formal and precise understanding of humans' conceptual scheme of computer programs and to improve the design of programming languages in order to allow humans to effectively communicate with computers. To understand programming languages and communicate with machines via these languages, it is important to study the \emph{meaning} of programs expressed using these languages, i.e., the semantics of programming languages.

% Due to my nature of stupidity, I am in no position to invent yet another way to study the meaning of programming languages. Instead, in my three useless and insignificant projects, I merely ask three conceptual questions and attempt to answer them via studying and making use of existing formal semantics models.

\begin{center}
\vspace{-0.7em}
\pgfornament[width=0.08*\framesize]{11}
\pgfornament[width=0.08*\framesize]{80}
\pgfornament[width=0.08*\framesize]{14}
\vspace{-0.3em}
\end{center}

The first conceptual question asked is: 
How to design a better abstraction mechanism that allows programmers to effectively express \emph{what} they want a computer to do via some declarative yet accurate \emph{specifications} instead of \emph{how} a computer should accomplish a task via some concrete \emph{implementations}?
% in order to achieve better automation?

To address this question, a topic has been taken as an instance to study, which is container types in programming languages and their properties. 

Container data types are ubiquitous in computer programming, enabling developers to efficiently store and process collections of data with an easy-to-use programming interface.
Many programming languages offer a variety of container implementations in their standard libraries based on data structures offering different capabilities and performance characteristics.
However, choosing the \emph{best} container for an application is not always straightforward, as performance characteristics can change drastically in different scenarios, and as real-world performance is not always correlated to theoretical complexity. Based on this observation, we bring up a research question: How to design a notion of container that truly allows to separate its interface and usage from the implementation and to infer the implementation from its interface and usage?

This question is answered by the project --- \emph{\Primrose{}: Selecting Container Data Types by Their Properties.}
In this project, we present \Primrose{}, a language-agnostic tool for selecting the best performing valid container implementation from a set of container data types that satisfy \emph{properties} given by application developers.
\Primrose{} automatically selects the set of valid container implementations for which the \emph{library specifications}, written by the developers of container libraries, satisfies the specified properties.
Finally, \Primrose{} ranks the valid library implementations based on their runtime performance.
With \Primrose{}, application developers can specify the expected behaviour of a container as a type refinement with \emph{semantic properties}, e.g., if the container should only contain unique values (such as a \lstinline{set}) or should satisfy the LIFO property of a \lstinline{stack}.
Semantic properties nicely complement \emph{syntactic properties} (i.e., traits, interfaces, or type classes), together allowing developers to specify a container's programming \emph{interface} and \emph{behaviour} without committing to a concrete implementation.
We present our prototype implementation of \Primrose{} that preprocesses annotated Rust code, selects valid container implementations and ranks them on their performance. The design of \Primrose{} is, however, language-agnostic, and is easy to integrate into other programming languages that support container data types and traits, interfaces, or type classes. Our implementation encodes properties and library specifications into verification conditions in Rosette, an interface for SMT solvers, which determines the set of valid container implementations. We evaluate \Primrose{} by specifying several container implementations, and measuring the time taken to select valid implementations for various combinations of properties with the solver. We automatically validate that container implementations conform to their library specifications via property-based testing.
This work provides a novel approach to bring abstract modelling and specification of container types directly into the programmer's workflow.
Instead of selecting concrete container implementations, application programmers can now work on the level of specification, merely stating the behaviours they require from their container types, and the best implementation can be selected automatically. In chapter~\ref{chap2}, we discuss this project in detail.

\begin{center}
\vspace{-0.7em}
\pgfornament[width=0.08*\framesize]{11}
\pgfornament[width=0.08*\framesize]{80}
\pgfornament[width=0.08*\framesize]{14}
\vspace{-0.3em}
\end{center}

The second conceptual question asked is: How to intuitively understand \emph{distributed programs} using the same conceptual model as \emph{monolithic programs}?
% How to get a intuitive understanding of distributed programs in order to simplify the process of migrating monolithic programs into distributed setting?

To address this question, a topic has been looked into, which is designing a remote procedural call library for Rust, allowing monolithic programs to be migrated into a distributed setting without massive re-coding, in the meanwhile extends Rust's memory safety guarantees into the distributed setting.

In distributed computing, a remote procedure call (RPC) allows a method invocation to be executed on another computer on a shared network. Such a remote method invocation has the same coding as a local method invocation, without the programmer explicitly encoding the details for the remote interaction. It is hard to support \emph{location transparency}, i.e., in existing RPC frameworks such as Java RMI and Rust tarpc, remote method invocations do not have \emph{the same semantics} as local method invocations. In addition, \emph{memory management} is hard in a distributed setting, for instance, distributed garbage collection is complicated.

To address these issues, we design a \emph{universal method invocation} (UMI) library in Rust supporting location transparency. With the UMI library, syntactically, a distributed program is written (almost) the same as a monolithic program; semantically, a distributed program preserves the semantics of a monolithic program. We choose Rust as a the target language for the UMI framework to utilise Rust's memory safety guarantees. Rust is high-level system programming language which \emph{guarantees memory safety} and \emph{prevents data races} by its \emph{ownership and borrow checking system} for memory management and tracking object lifetime of all references in a program during compilation. Since Rust has semantics that guarantees memory safety, we can extend such guarantees to the distributed computing setting, allowing our UMI framework to provide safe remote method invocations.

We provide a usable \emph{Rust implementation} of the UMI framework and formalise the \emph{small-step operational semantics} for a core calculus of monolithic and distributed Rust programs. In addition, we prove a \emph{location transparency theorem}: With the UMI framework, when a monolithic program is deployed to multiple nodes, its semantics is preserved.
This project is discussed in detail in chapter~\ref{chap3}.

\begin{center}
\vspace{-0.7em}
\pgfornament[width=0.08*\framesize]{11}
\pgfornament[width=0.08*\framesize]{80}
\pgfornament[width=0.08*\framesize]{14}
\vspace{-0.3em}
\end{center}

The third conceptual question asked is: 
How do we characterise the relationship between the \emph{syntax} and \emph{semantics} of programming languages?
% How to gain a formal understanding of and reason about a programming language that performs syntactic transformation? 

I am certainly not the first person asking such a question. In fact, in the world of linguistic studies, the ``linguistics wars"~\citep{alma993219653502466} happened in 60s and 70s were a academic dispute on the relationship between the syntax and semantics of natural languages. Dating back to the 50s, by presenting the sentence ``Colourless green ideas sleep furiously", which is grammatically correct but nonsensical, \citet{Chomsky+1957}
argues that the syntax of a language being independent from the semantics. However, some structural linguists emphasise that the analysis of language structure and meaning should be within a synchronic framework.

In programming languages studies, it is generally agreed that the syntax, which represents the form of programs, organises the symbols and defines the programs' structure without giving the meaning to the programs. After defining the syntax, the semantics, which is the meaning, can then be assigned to syntactic valid programs, by describing the execution of programs.

While the syntax and semantics concern different aspects of the design of programming languages, they are not completely independent. In the two previous studies, we have already discussed designs concerning syntactic properties of collections of data and minimising the changes in syntax while changing the architecture of programs. There are some perhaps conceptually important observations of the design of the syntactic contracts. Firstly, although comparing to modelling semantic properties and reasoning about the semantic preservation, these syntactic constructs seems to be too straightforward to discuss in detail and it is not straightforward to evaluate how ``well" they have been designed, they are still very important serving as an abstraction that allows and facilitates programmers to convey the intend semantics of programs. In \Primrose{}, we have seen that some semantic properties like LIFO depend on the syntactic properties, giving a characterisation of a container by stating the behaviours of specific operations given by syntactic properties. In the design of the UMI framework, the syntax serves as the abstraction for location transparency, where the location of resources is abstracted away from how programmers interact with them, while the semantics gives meaning to such an abstraction utilising which we are able to reason about that resources are properly used and managed independent from where they are stored.

In the third project, we studied the syntax and semantics of programming languages from yet another perspective, via the rewriting.
Rewriting is a versatile and powerful technique used in many domains including symbolic computation, theorem proving, programming language semantics, and compiler optimisation.
While being practically useful, rewriting is also conceptually intriguing. In a rewriting system, syntactic transformations is used to systematically encode the semantics of the reduction, simplification and evaluation of expressions. Since these syntactic transformation steps are composible, valid composition of valid syntactic transformation steps form a meaningful program, the process of composing syntactic transformation steps together has its own semantics to be examined. 

In practice, \emph{strategic rewriting} is a systematic technique that allows programmers to control the application of rewrite rules by composing individual rewrite rules into complex rewrite strategies. These strategies have concise and intuitive syntactic constructs and simply serve as compositions of syntactic transformations but are semantically complex, as they may be nondeterministic, they may raise errors that trigger backtracking, and they may not terminate.
% System S, which is a core calculus of strategic rewriting languages like Stratego, Elevate, and Strafunski, as the subject to study. We have developed denotational semantics, big-step operational semantics, and axiomatic semantics (i.e., the weakest precondition calculus) for System S in order to formally understand and reason about such a family of strategic rewriting languages.
% Rewriting is a versatile and powerful technique used in many domains.
% \emph{Strategic rewriting} allows programmers to control the application of rewrite rules by composing individual rewrite rules into complex rewrite strategies. These strategies are semantically complex, as they may be nondeterministic, they may raise errors that trigger backtracking, and they may not terminate.
Given such semantic complexity, it is necessary to establish a formal understanding of rewrite strategies and to enable reasoning about them in order to answer questions such as:
How do we characterise errors and divergence in a strategic rewriting system?
How do we understand and model nondeterminism in the executions of strategies?
How do we know that a rewrite strategy terminates?
How do we know that a rewrite strategy does not fail because we compose two incompatible rewrites?
How do we know that a desired property holds after applying a rewrite strategy?

These questions are answered by the project --- \emph{Shoggoth: A Formal Foundation for Strategic Rewriting}. It provides a semantic foundation for understanding, analysing and reasoning about strategic rewriting that is capable of answering these questions.
We provide a denotational semantics of System S, which is a core calculus of strategic rewriting languages like Stratego~\citep{DBLP:conf/icfp/VisserBT98,10.1007/3-540-45127-7_27}, Elevate~\citep{DBLP:journals/cacm/HagedornLKQGS23,DBLP:journals/pacmpl/HagedornLKQGS20}, and Strafunski~\citep{DBLP:conf/rule/LammelV02}, and prove its equivalence to our big-step operational semantics, which extends existing work by explicitly accounting for divergence.
We further define a \emph{location-based weakest precondition calculus}, which can be seen as an axiomatic semantics of System S~\cite{VISSER1998422}, to enable formal reasoning about rewriting strategies, and we prove this calculus sound with respect to the denotational semantics.
We show how this calculus can be used in practice to reason about properties of rewriting strategies, including termination, that they are well-composed, and that desired postconditions hold.
The semantics and calculus are formalised in Isabelle/HOL and all proofs are mechanised. This project is discussed in detail in chapter~\ref{chap4}.



% This thesis contains three studies which explore different aspects of modelling and reasoning about programming languages:
% \begin{itemize}
%     \item The first study is designing property-based container types that allow application programmers to specify their desired properties of a container, enabling a ``best-performing" implementation to be selected from a container library accordingly. (Chapter~\ref{chap2})
%     \item The second study is the design and implementation of a remote procedural call library in Rust. (Chapter~\ref{chap3})
%     \item The third study contains the formal semantics of a strategy rewriting language: System S as well as, a weakest precondition calculus for reasoning about strategic rewriting. (Chapter~\ref{chap4})
% \end{itemize}
